{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Tags",
    "colab": {
      "name": "Training_HieRec_MIND_Large_20211.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uejUZZ-JX4pW",
        "K4hUiZcdJF7L",
        "XXoX58I1ZWEn",
        "e6qfS4BsX-zI",
        "oFb8Wo8PYKXw",
        "gdGkdPt9YVL_",
        "y3vuhkWGzpCN",
        "HjtKktNvzylz",
        "GhczrSvO1ISA"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv5BuZNfI-h-"
      },
      "source": [
        "!pip uninstall tensorflow==2.7.0 -y\n",
        "!pip install -U keras==2.2.4 tensorflow-gpu==1.14.0 h5py==2.10.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxPnjZhKKrha"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)\n",
        "print(tensorflow.test.gpu_device_name())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uejUZZ-JX4pW"
      },
      "source": [
        "### Hyperparams (hypers.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAdisPGGX8wo"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "npratio = 4\n",
        "MAX_SENTENCE = 30\n",
        "MAX_ALL = 50\n",
        "MAX_SENT_LENGTH=30\n",
        "MAX_SENTS=50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4hUiZcdJF7L"
      },
      "source": [
        "### Additional functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAIKQmJxJNpn"
      },
      "source": [
        "def get_raw_entity_list_dict():\n",
        "    raw_entity_list_dict = []\n",
        "    set_news = set()\n",
        "\n",
        "    with open('/content/data/train_set_large/news.tsv') as f:\n",
        "            a_lines = f.readlines()\n",
        "    with open('/content/data/val_set_large/news.tsv') as f:\n",
        "            a_lines += f.readlines()\n",
        "    with open('/content/data/test_set_large/news.tsv') as f:\n",
        "            a_lines += f.readlines()\n",
        "\n",
        "    for l in a_lines:\n",
        "        l = l.strip().split('\\t')\n",
        "        if l[0] not in set_news:\n",
        "            raw_entity_list_dict.append({\"doc_id\":l[0],\"entities\":json.loads(l[-2])})\n",
        "            set_news.add(l[0])\n",
        "    return raw_entity_list_dict\n",
        "\n",
        "\n",
        "def mini_get_raw_entity_list_dict():\n",
        "    raw_entity_list_dict = []\n",
        "    set_news = set()\n",
        "\n",
        "    with open('/content/data/train_set/news.tsv') as f:\n",
        "            a_lines = f.readlines()\n",
        "    with open('/content/data/val_set/news.tsv') as f:\n",
        "            a_lines += f.readlines()\n",
        "\n",
        "    for l in a_lines:\n",
        "        l = l.strip().split('\\t')\n",
        "        if l[0] not in set_news:\n",
        "            raw_entity_list_dict.append({\"doc_id\":l[0],\"entities\":json.loads(l[-2])})\n",
        "            set_news.add(l[0])\n",
        "    return raw_entity_list_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwqSzvrmPK00"
      },
      "source": [
        "def get_raw_pretrained_entity_embedding():\n",
        "    import numpy as np\n",
        "    raw_entity_embedding = {}\n",
        "    set_entity = set()\n",
        "\n",
        "    with open('/content/data/train_set_large/entity_embedding.vec','r') as f:\n",
        "        tmp_a = f.readlines()\n",
        "\n",
        "    with open('/content/data/val_set_large/entity_embedding.vec','r') as f:\n",
        "        tmp_a += f.readlines()\n",
        "\n",
        "    with open('/content/data/test_set_large/entity_embedding.vec','r') as f:\n",
        "        tmp_a += f.readlines()\n",
        "\n",
        "    for l in tmp_a:\n",
        "        l = l.strip().split('\\t')\n",
        "        if l[0] not in set_entity:\n",
        "            set_entity.add(l[0])\n",
        "            raw_entity_embedding[l[0]] = np.array(l[1:]).astype(np.float)\n",
        "\n",
        "    return raw_entity_embedding\n",
        "\n",
        "\n",
        "def mini_get_raw_pretrained_entity_embedding():\n",
        "    import numpy as np\n",
        "    raw_entity_embedding = {}\n",
        "    set_entity = set()\n",
        "\n",
        "    with open('data/train_set/entity_embedding.vec','r') as f:\n",
        "        tmp_a = f.readlines()\n",
        "\n",
        "    with open('/content/data/val_set/entity_embedding.vec','r') as f:\n",
        "        tmp_a += f.readlines()\n",
        "        \n",
        "    for l in tmp_a:\n",
        "        l = l.strip().split('\\t')\n",
        "        if l[0] not in set_entity:\n",
        "            set_entity.add(l[0])\n",
        "            raw_entity_embedding[l[0]] = np.array(l[1:]).astype(np.float)\n",
        "\n",
        "    return raw_entity_embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXoX58I1ZWEn"
      },
      "source": [
        "### Data preparation (ProcessRawData.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWfoqY4Ua-g3"
      },
      "source": [
        "!wget https://mind201910small.blob.core.windows.net/release/MINDlarge_train.zip -P data/train_set_large/\n",
        "!wget https://mind201910small.blob.core.windows.net/release/MINDlarge_dev.zip -P data/val_set_large/\n",
        "!wget https://mind201910small.blob.core.windows.net/release/MINDlarge_test.zip -P data/test_set_large/\n",
        "!unzip data/test_set_large/MINDlarge_test.zip -d data/test_set_large/\n",
        "!unzip data/train_set_large/MINDlarge_train.zip -d data/train_set_large/\n",
        "!unzip data/val_set_large/MINDlarge_dev.zip -d data/val_set_large/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeFXI6RCZuEa"
      },
      "source": [
        "# GloVe\n",
        "!wget https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "!unzip /content/glove.840B.300d.zip -d /content/embedding/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MIND Large\n",
        "raw_data_path = './data'\n",
        "output_data_path = './input_data'\n",
        "\n",
        "if os.path.exists(output_data_path) == False:\n",
        "  os.mkdir(output_data_path)\n",
        "\n",
        "with open(os.path.join(raw_data_path,'train_set_large','news.tsv')) as f:\n",
        "    news1 = f.readlines()\n",
        "with open(os.path.join(raw_data_path,'val_set_large','news.tsv')) as f:\n",
        "    news2 = f.readlines()\n",
        "with open(os.path.join(raw_data_path,'test_set_large','news.tsv')) as f:\n",
        "    news3 = f.readlines()\n",
        "\n",
        "news = []\n",
        "news_dict = {}\n",
        "for l in news1 + news2 + news3:\n",
        "    nid = l.strip('\\n').split('\\t')[0]\n",
        "    if not nid in news_dict:\n",
        "        news_dict[nid] = 1\n",
        "        news.append(l)\n",
        "\n",
        "with open(os.path.join(output_data_path,'docs.tsv'),'w') as f:\n",
        "    for i in range(len(news)):\n",
        "        f.write(news[i])\n",
        "\n",
        "with open(os.path.join(raw_data_path,'train_set_large','behaviors.tsv')) as f:\n",
        "    behaviors1 = f.readlines()\n",
        "with open(os.path.join(raw_data_path,'val_set_large','behaviors.tsv')) as f:\n",
        "    behaviors2 = f.readlines()\n",
        "with open(os.path.join(raw_data_path,'test_set_large','behaviors.tsv')) as f:\n",
        "    behaviors3 = f.readlines()\n",
        "\n",
        "# train data + all val data for training, 10% val data for validation (or rather self-testing), test data for submission\n",
        "train_behaviors = []\n",
        "val_behaviors = []\n",
        "test_behaviors = []\n",
        "# num for fun\n",
        "num = int(0.9*len(behaviors2))\n",
        "\n",
        "# Get train data (behaviors of all train data + val data)\n",
        "for i in range(len(behaviors1)):\n",
        "    train_behaviors.append(behaviors1[i])\n",
        "for i in range(len(behaviors2)):\n",
        "    train_behaviors.append(behaviors2[i])\n",
        "\n",
        "# Modify training portion\n",
        "start_portion = int(len(train_behaviors)*0.6)\n",
        "end_portion = int(len(train_behaviors)*1)\n",
        "train_behaviors = train_behaviors[start_portion:end_portion]\n",
        "print(f\"Training start from portion {start_portion} to {end_portion}\")\n",
        "\n",
        "# Get test data\n",
        "# Remember this is MIND Large (test data doesn't contain labels)\n",
        "test_behaviors = behaviors3\n",
        "\n",
        "with open(os.path.join(output_data_path,'train.tsv'),'w') as f:\n",
        "    for i in range(len(train_behaviors)):\n",
        "        f.write(train_behaviors[i])\n",
        "with open(os.path.join(output_data_path,'val.tsv'),'w') as f:\n",
        "    for i in range(len(val_behaviors)):\n",
        "        f.write(val_behaviors[i])\n",
        "with open(os.path.join(output_data_path,'test.tsv'),'w') as f:\n",
        "    for i in range(len(test_behaviors)):\n",
        "        f.write(test_behaviors[i])"
      ],
      "metadata": {
        "id": "PTvdFB0ksuot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5ae422a-d759-4523-d78e-11cec2dfcfb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training start from portion 1565531 to 2609219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6qfS4BsX-zI"
      },
      "source": [
        "### Preprocessing (preprocessing.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzRMYXxlZ16S"
      },
      "source": [
        "data_root_path = '/content/input_data/'\n",
        "embedding_path = '/content/embedding/'\n",
        "KG_root_path = '/content/HieRec_KGData'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BHIuYIsYDH-"
      },
      "source": [
        "from datetime import datetime\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "import json\n",
        "\n",
        "def trans2tsp(timestr):\n",
        "    return int(time.mktime(datetime.strptime(timestr, '%m/%d/%Y %I:%M:%S %p').timetuple()))\n",
        "\n",
        "def newsample(nnn,ratio):\n",
        "    if ratio >len(nnn):\n",
        "        return random.sample(nnn*(ratio//len(nnn)+1),ratio)\n",
        "    else:\n",
        "        return random.sample(nnn,ratio)\n",
        "\n",
        "def shuffle(pn,labeler,pos):\n",
        "    index=np.arange(pn.shape[0])\n",
        "    pn=pn[index]\n",
        "    labeler=labeler[index]\n",
        "    pos=pos[index]\n",
        "    \n",
        "    for i in range(pn.shape[0]):\n",
        "        index=np.arange(npratio+1)\n",
        "        pn[i,:]=pn[i,index]\n",
        "        labeler[i,:]=labeler[i,index]\n",
        "    return pn,labeler,pos\n",
        "\n",
        "def read_news(path,filenames):\n",
        "    news={}\n",
        "    category=[]\n",
        "    subcategory=[]\n",
        "    news_index={}\n",
        "    index=1\n",
        "    word_dict={}\n",
        "    word_index=1\n",
        "    with open(os.path.join(path,filenames)) as f:\n",
        "        lines=f.readlines()\n",
        "    for line in lines:\n",
        "        splited = line.strip('\\n').split('\\t')\n",
        "        doc_id,vert,subvert,title= splited[0:4]\n",
        "        news_index[doc_id]=index\n",
        "        index+=1\n",
        "        category.append(vert)\n",
        "        subcategory.append(subvert)\n",
        "        title = title.lower()\n",
        "        title=word_tokenize(title)\n",
        "        news[doc_id]=[vert,subvert,title]\n",
        "        for word in title:\n",
        "            word = word.lower()\n",
        "            if not(word in word_dict):\n",
        "                word_dict[word]=word_index\n",
        "                word_index+=1\n",
        "    category=list(set(category))\n",
        "    subcategory=list(set(subcategory))\n",
        "    category_dict={}\n",
        "    index=1\n",
        "    for c in category:\n",
        "        category_dict[c]=index\n",
        "        index+=1\n",
        "    subcategory_dict={}\n",
        "    index=1\n",
        "    for c in subcategory:\n",
        "        subcategory_dict[c]=index\n",
        "        index+=1\n",
        "    return news,news_index,category_dict,subcategory_dict,word_dict\n",
        "\n",
        "def get_doc_input(news,news_index,category,subcategory,word_dict):\n",
        "    news_num=len(news)+1\n",
        "    news_title=np.zeros((news_num,MAX_SENTENCE),dtype='int32')\n",
        "    news_vert=np.zeros((news_num,),dtype='int32')\n",
        "    news_subvert=np.zeros((news_num,),dtype='int32')\n",
        "    for key in news:    \n",
        "        vert,subvert,title=news[key]\n",
        "        doc_index=news_index[key]\n",
        "        news_vert[doc_index]=category[vert]\n",
        "        news_subvert[doc_index]=subcategory[subvert]\n",
        "        for word_id in range(min(MAX_SENTENCE,len(title))):\n",
        "            news_title[doc_index,word_id]=word_dict[title[word_id].lower()]\n",
        "    return news_title,news_vert,news_subvert\n",
        "\n",
        "def load_matrix(embedding_path,word_dict):\n",
        "    embedding_matrix = np.zeros((len(word_dict)+1,300))\n",
        "    have_word=[]\n",
        "    with open(os.path.join(embedding_path,'glove.840B.300d.txt'),'rb') as f:\n",
        "        while True:\n",
        "            l=f.readline()\n",
        "            if len(l)==0:\n",
        "                break\n",
        "            l=l.split()\n",
        "            word = l[0].decode()\n",
        "            if word in word_dict:\n",
        "                index = word_dict[word]\n",
        "                tp = [float(x) for x in l[1:]]\n",
        "                embedding_matrix[index]=np.array(tp)\n",
        "                have_word.append(word)\n",
        "    return embedding_matrix,have_word\n",
        "\n",
        "def read_clickhistory(news_index,data_root_path,filename):\n",
        "    \n",
        "    lines = []\n",
        "    userids = []\n",
        "    with open(os.path.join(data_root_path,filename)) as f:\n",
        "        lines = f.readlines()\n",
        "        \n",
        "    sessions = []\n",
        "    for i in range(len(lines)):\n",
        "        _,uid,eventime, click, imps = lines[i].strip().split('\\t')\n",
        "        if click == '':\n",
        "            clikcs = []\n",
        "        else:\n",
        "            clikcs = click.split()\n",
        "        true_click = []\n",
        "        for click in clikcs:\n",
        "            if not click in news_index:\n",
        "                continue\n",
        "            true_click.append(click)\n",
        "        pos = []\n",
        "        neg = []\n",
        "        for imp in imps.split():\n",
        "            docid, label = imp.split('-')\n",
        "            if label == '1':\n",
        "                pos.append(docid)\n",
        "            else:\n",
        "                neg.append(docid)\n",
        "        sessions.append([true_click,pos,neg])\n",
        "    return sessions\n",
        "\n",
        "def parse_user(news_index,session):\n",
        "    user_num = len(session)\n",
        "    user={'click': np.zeros((user_num,MAX_ALL),dtype='int32'),}\n",
        "    for user_id in range(len(session)):\n",
        "        tclick = []\n",
        "        click, pos, neg =session[user_id]\n",
        "        for i in range(len(click)):\n",
        "            tclick.append(news_index[click[i]])\n",
        "        click = tclick\n",
        "\n",
        "        if len(click) >MAX_ALL:\n",
        "            click = click[-MAX_ALL:]\n",
        "        else:\n",
        "            click=[0]*(MAX_ALL-len(click)) + click\n",
        "            \n",
        "        user['click'][user_id] = np.array(click)\n",
        "    return user\n",
        "\n",
        "def get_train_input(news_index,session):\n",
        "    sess_pos = []\n",
        "    sess_neg = []\n",
        "    user_id = []\n",
        "    for sess_id in range(len(session)):\n",
        "        sess = session[sess_id]\n",
        "        _, poss, negs=sess\n",
        "        for i in range(len(poss)):\n",
        "            pos = poss[i]\n",
        "            neg=newsample(negs,npratio)\n",
        "            sess_pos.append(pos)\n",
        "            sess_neg.append(neg)\n",
        "            user_id.append(sess_id)\n",
        "    sess_all = np.zeros((len(sess_pos),1+npratio),dtype='int32')\n",
        "    label = np.zeros((len(sess_pos),1+npratio))\n",
        "    for sess_id in range(sess_all.shape[0]):\n",
        "        pos = sess_pos[sess_id]\n",
        "        negs = sess_neg[sess_id]\n",
        "        sess_all[sess_id,0] = news_index[pos]\n",
        "        index = 1\n",
        "        for neg in negs:\n",
        "            sess_all[sess_id,index] = news_index[neg]\n",
        "            index+=1\n",
        "        label[sess_id,0]=1\n",
        "    user_id = np.array(user_id, dtype='int32')\n",
        "    \n",
        "    return sess_all, user_id, label\n",
        "\n",
        "def get_test_input(news_index,session):\n",
        "    \n",
        "    Impressions = []\n",
        "    userid = []\n",
        "    for sess_id in range(len(session)):\n",
        "        _, poss, negs = session[sess_id]\n",
        "        imp = {'labels':[],\n",
        "                'docs':[]}\n",
        "        userid.append(sess_id)\n",
        "        for i in range(len(poss)):\n",
        "            docid = news_index[poss[i]]\n",
        "            imp['docs'].append(docid)\n",
        "            imp['labels'].append(1)\n",
        "        for i in range(len(negs)):\n",
        "            docid = news_index[negs[i]]\n",
        "            imp['docs'].append(docid)\n",
        "            imp['labels'].append(0)\n",
        "        Impressions.append(imp)\n",
        "        \n",
        "    userid = np.array(userid,dtype='int32')\n",
        "    \n",
        "    return Impressions, userid,\n",
        "\n",
        "def load_news_entity(news_index,KG_root_path):\n",
        "\n",
        "    lines = get_raw_entity_list_dict()\n",
        "    \n",
        "    EntityId2Index = {}\n",
        "    ctt = 1\n",
        "    \n",
        "    news_entity = {}\n",
        "    g = []\n",
        "    for i in range(len(lines)):\n",
        "        d = lines[i]\n",
        "        docid = d['doc_id']\n",
        "        if not docid in news_index:\n",
        "            continue\n",
        "        news_entity[docid] = []\n",
        "        entities = d['entities']\n",
        "        for j in range(len(entities)):\n",
        "            e = entities[j]['Label']\n",
        "            eid = entities[j]['WikidataId']\n",
        "            if not eid in EntityId2Index:\n",
        "                EntityId2Index[eid] = ctt\n",
        "                ctt += 1    \n",
        "            news_entity[docid].append([e,eid,EntityId2Index[eid]])\n",
        "    \n",
        "    meta_news_entity = {}\n",
        "    news_entity2 = {}\n",
        "    \n",
        "    \n",
        "    news_entity_id = {}\n",
        "    for nid in news_entity:\n",
        "        news_entity_id[nid] = []\n",
        "        for e in news_entity[nid]:\n",
        "            news_entity_id[nid].append(e[-2])\n",
        "        news_entity_id[nid] = set(news_entity_id[nid])\n",
        "        \n",
        "    \n",
        "    for docid in news_entity:\n",
        "        meta_news_entity[docid] = news_entity[docid]\n",
        "        news_entity2[docid] = []\n",
        "        for v in news_entity[docid]:\n",
        "            news_entity2[docid].append(v[-1])\n",
        "        news_entity2[docid] = list(set(news_entity2[docid]))[:5]\n",
        "        news_entity2[docid] = news_entity2[docid] + [0]*(5-len(news_entity2[docid]))\n",
        "        news_entity2[docid] = np.array(news_entity2[docid])\n",
        "    \n",
        "    news_entity_np = np.zeros((len(news_entity2)+1,5),dtype='int32')\n",
        "    for nid in news_index:\n",
        "        nix = news_index[nid]\n",
        "        news_entity_np[nix] = news_entity2[nid]\n",
        "        \n",
        "    return news_entity_id,news_entity_np,EntityId2Index\n",
        "\n",
        "def load_entity_embedding(KG_root_path,EntityId2Index):\n",
        "    entity_emb = np.zeros((len(EntityId2Index)+1,100))\n",
        "    title_entity_emb = get_raw_pretrained_entity_embedding()\n",
        "    \n",
        "    for eid in EntityId2Index:\n",
        "        if eid in title_entity_emb:\n",
        "            eix = EntityId2Index[eid]\n",
        "            entity_emb[eix] = title_entity_emb[eid]\n",
        "    return entity_emb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFb8Wo8PYKXw"
      },
      "source": [
        "### Utils (utils.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eg4VupZYLj-"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "def dcg_score(y_true, y_score, k=10):\n",
        "    order = np.argsort(y_score)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "    gains = 2 ** y_true - 1\n",
        "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
        "    return np.sum(gains / discounts)\n",
        "\n",
        "\n",
        "def ndcg_score(y_true, y_score, k=10):\n",
        "    best = dcg_score(y_true, y_true, k)\n",
        "    actual = dcg_score(y_true, y_score, k)\n",
        "    return actual / best\n",
        "\n",
        "\n",
        "def mrr_score(y_true, y_score):\n",
        "    order = np.argsort(y_score)[::-1]\n",
        "    y_true = np.take(y_true, order)\n",
        "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
        "    return np.sum(rr_score) / np.sum(y_true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdGkdPt9YVL_"
      },
      "source": [
        "### Models (models.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvwBdVLHYZrn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b73fa7e6-60a7-4db0-d815-5851e46a7aa6"
      },
      "source": [
        "import numpy\n",
        "import keras\n",
        "from keras.utils.np_utils import *\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "from keras.layers import Embedding, concatenate\n",
        "from keras.layers import Dense, Input, Flatten, average,Lambda\n",
        "\n",
        "from keras.layers import *\n",
        "from keras.models import Model, load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer, InputSpec\n",
        "from keras import initializers \n",
        "from keras.utils.vis_utils import plot_model\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from keras.optimizers import *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VfkPfQvYhD4"
      },
      "source": [
        "class Attention(Layer):\n",
        " \n",
        "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
        "        self.nb_head = nb_head\n",
        "        self.size_per_head = size_per_head\n",
        "        self.output_dim = nb_head*size_per_head\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        " \n",
        "    def build(self, input_shape):\n",
        "        self.WQ = self.add_weight(name='WQ',\n",
        "                                  shape=(input_shape[0][-1], self.output_dim),\n",
        "                                  initializer='glorot_uniform',\n",
        "                                  trainable=True)\n",
        "        self.WK = self.add_weight(name='WK',\n",
        "                                  shape=(input_shape[1][-1], self.output_dim),\n",
        "                                  initializer='glorot_uniform',\n",
        "                                  trainable=True)\n",
        "        self.WV = self.add_weight(name='WV',\n",
        "                                  shape=(input_shape[2][-1], self.output_dim),\n",
        "                                  initializer='glorot_uniform',\n",
        "                                  trainable=True)\n",
        "        super(Attention, self).build(input_shape)\n",
        " \n",
        "    def Mask(self, inputs, seq_len, mode='mul'):\n",
        "        if seq_len == None:\n",
        "            return inputs\n",
        "        else:\n",
        "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
        "            mask = 1 - K.cumsum(mask, 1)\n",
        "            for _ in range(len(inputs.shape)-2):\n",
        "                mask = K.expand_dims(mask, 2)\n",
        "            if mode == 'mul':\n",
        "                return inputs * mask\n",
        "            if mode == 'add':\n",
        "                return inputs - (1 - mask) * 1e12\n",
        " \n",
        "    def call(self, x):\n",
        "        if len(x) == 3:\n",
        "            Q_seq,K_seq,V_seq = x\n",
        "            Q_len,V_len = None,None\n",
        "        elif len(x) == 5:\n",
        "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
        "\n",
        "        Q_seq = K.dot(Q_seq, self.WQ)\n",
        "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
        "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
        "        K_seq = K.dot(K_seq, self.WK)\n",
        "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
        "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
        "        V_seq = K.dot(V_seq, self.WV)\n",
        "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
        "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
        "\n",
        "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
        "        A = K.permute_dimensions(A, (0,3,2,1))\n",
        "        A = self.Mask(A, V_len, 'add')\n",
        "        A = K.permute_dimensions(A, (0,3,2,1))\n",
        "        A = K.softmax(A)\n",
        "\n",
        "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n",
        "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n",
        "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n",
        "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
        "        return O_seq\n",
        " \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0][0], input_shape[0][1], self.output_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuAD6sonYlqj"
      },
      "source": [
        "def AttentivePooling(dim1,dim2):\n",
        "    vecs_input = Input(shape=(dim1,dim2),dtype='float32')\n",
        "    user_vecs =Dropout(0.2)(vecs_input)\n",
        "    user_att = Dense(200,activation='tanh')(user_vecs)\n",
        "    user_att = keras.layers.Flatten()(Dense(1)(user_att))\n",
        "    user_att = Activation('softmax')(user_att)\n",
        "    user_vec = keras.layers.Dot((1,1))([user_vecs,user_att])\n",
        "    model = Model(vecs_input,user_vec)\n",
        "    return model\n",
        "\n",
        "def ConDot():\n",
        "    vec_input = keras.layers.Input(shape=(400*2,))\n",
        "    vec1 = keras.layers.Lambda(lambda x:x[:,:400])(vec_input)\n",
        "    vec2 = keras.layers.Lambda(lambda x:x[:,400:])(vec_input)\n",
        "    score = keras.layers.Dot(axes=-1)([vec1,vec2])\n",
        "    return Model(vec_input,score)\n",
        "\n",
        "def get_doc_encoder(title_word_embedding_matrix,entity_emb_matrix):\n",
        "\n",
        "    news_input = Input(shape=(35,),dtype='int32')\n",
        "    \n",
        "    \n",
        "    sentence_input = keras.layers.Lambda(lambda x:x[:,:30])(news_input)\n",
        "    title_word_embedding_layer = Embedding(title_word_embedding_matrix.shape[0], 300, weights=[title_word_embedding_matrix],trainable=True)\n",
        "    word_vecs = title_word_embedding_layer(sentence_input)\n",
        "    droped_vecs = Dropout(0.2)(word_vecs)\n",
        "    word_rep = Attention(20,20)([droped_vecs]*3)\n",
        "    droped_rep = Dropout(0.2)(word_rep)\n",
        "    title_vec = AttentivePooling(30,400)(droped_rep)\n",
        "    \n",
        "    entity_input = keras.layers.Lambda(lambda x:x[:,30:])(news_input)\n",
        "    entity_embedding_layer = Embedding(entity_emb_matrix.shape[0], 100, weights=[entity_emb_matrix],trainable=True)\n",
        "    entity_vecs = entity_embedding_layer(entity_input)\n",
        "    droped_vecs = Dropout(0.2)(entity_vecs)\n",
        "    entity_rep = Attention(5,20)([droped_vecs]*3)\n",
        "    droped_rep = Dropout(0.2)(entity_rep)\n",
        "    entity_vec = AttentivePooling(5,100)(droped_rep)\n",
        "    \n",
        "    vec = keras.layers.Concatenate(axis=-1)([title_vec,entity_vec])\n",
        "    vec = keras.layers.Dense(400)(vec)\n",
        "    \n",
        "    \n",
        "    sentEncodert = Model(news_input, vec)\n",
        "    return sentEncodert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2zqDMPhYsQa"
      },
      "source": [
        "class CategoryEmbLayer(Layer):\n",
        "    \n",
        "    def __init__(self,n, **kwargs):\n",
        "        super(CategoryEmbLayer, self).__init__(**kwargs)\n",
        "        self.n = n\n",
        "        \n",
        "     \n",
        "    def build(self, input_shape):\n",
        "        trainable = True\n",
        "        if self.n>1:\n",
        "            self.W = self.add_weight(name='W',\n",
        "                                  shape=(self.n,400),\n",
        "                                  initializer=keras.initializers.Constant(value=np.zeros((self.n,400))),\n",
        "                                  trainable=trainable)\n",
        "        else:\n",
        "            self.W = self.add_weight(name='W',\n",
        "                                  shape=(400,),\n",
        "                                  initializer=keras.initializers.Constant(value=np.zeros((400,))),\n",
        "                                  trainable=trainable)\n",
        "            \n",
        "    def call(self,x):\n",
        "        return x+self.W\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "\n",
        "class Weighter(Layer):\n",
        "     \n",
        "    def __init__(self, **kwargs):\n",
        "        super(Weighter, self).__init__(**kwargs)\n",
        "        \n",
        "\n",
        "     \n",
        "    def build(self, input_shape):\n",
        "        trainable = False\n",
        "        self.w1 = self.add_weight(name='w1',\n",
        "                                  shape=(1,),\n",
        "                                  initializer=keras.initializers.Constant(value=0.15/0.15),\n",
        "                                  trainable=trainable)\n",
        "\n",
        "        self.w2 = self.add_weight(name='w2',\n",
        "                                  shape=(1,),\n",
        "                                  initializer=keras.initializers.Constant(value=0.15/0.15),\n",
        "                                  trainable=trainable)\n",
        "        \n",
        "        self.w3 = self.add_weight(name='w3',\n",
        "                                  shape=(1,),\n",
        "                                  initializer=keras.initializers.Constant(value=0.7/0.15),\n",
        "                                  trainable=trainable)\n",
        "\n",
        "        \n",
        "        super(Weighter, self).build(input_shape)\n",
        "        \n",
        "        \n",
        "    def call(self,x):\n",
        "\n",
        "        return self.w1*x[0]+self.w2*x[1]+self.w3*x[2]\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "\n",
        "        return input_shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StRHfPfBYyUl"
      },
      "source": [
        "def HirUserEncoder(category_dict,subcategory_dict):\n",
        "    \n",
        "    AttTrainable = True\n",
        "    \n",
        "    clicked_title_input = Input(shape=(50,400,), dtype='float32')\n",
        "    \n",
        "    clicked_vert_input = Input(shape=(len(category_dict),50,), dtype='float32')\n",
        "    clicked_vert_mask_input = Input(shape=(len(category_dict),), dtype='float32')\n",
        "    \n",
        "    clicked_subvert_input = Input(shape=(len(subcategory_dict),50,), dtype='float32')\n",
        "    clicked_subvert_mask_input = Input(shape=(len(subcategory_dict),), dtype='float32')\n",
        "    \n",
        "    vert_subvert_mask_input = Input(shape=(len(category_dict),len(subcategory_dict)),dtype='float32')\n",
        "\n",
        "    vert_num_input = Input(shape=(len(category_dict),),dtype='int32')\n",
        "    subvert_num_input = Input(shape=(len(subcategory_dict),),dtype='int32')\n",
        "\n",
        "    subvert_num_embedding_layer = Embedding(51, 128,trainable=True)\n",
        "    subvert_num_scorer = Dense(1)\n",
        "\n",
        "\n",
        "    vert_num_embedding_layer = subvert_num_embedding_layer #Embedding(51, 128,trainable=True)\n",
        "    vert_num_scorer = subvert_num_scorer\n",
        "\n",
        "    title_vecs = clicked_title_input\n",
        "    \n",
        "    trainable = True\n",
        "    \n",
        "    user_subvert_att = Dense(1,trainable=trainable,use_bias=False,kernel_initializer=keras.initializers.Constant(value=np.zeros((400,1))),)(title_vecs)\n",
        "\n",
        "    user_subvert_att = keras.layers.Reshape((50,))(user_subvert_att)\n",
        "    user_subvert_att = keras.layers.RepeatVector(len(subcategory_dict))(user_subvert_att)\n",
        "    user_subvert_att = keras.layers.Lambda(lambda x:x[0]-100*(1-x[1]))([user_subvert_att,clicked_subvert_input])    \n",
        "    user_subvert_att = keras.layers.Activation('softmax')(user_subvert_att) #(300,50)\n",
        "\n",
        "    user_subvert_att = keras.layers.Lambda(lambda x:x[0]*x[1])([user_subvert_att,clicked_subvert_input]) #(300,400)\n",
        "    user_subvert_rep = keras.layers.Dot(axes=[-1,-2])([user_subvert_att,title_vecs]) #（300,400)\n",
        "    user_subvert_rep = CategoryEmbLayer(len(subcategory_dict))(user_subvert_rep)  #（300,400) \n",
        "    \n",
        "    subvert_num_emb = subvert_num_embedding_layer(subvert_num_input)\n",
        "    subvert_num_score = subvert_num_scorer(subvert_num_emb)\n",
        "    subvert_num_score = Reshape((len(subcategory_dict),))(subvert_num_score) #(300,)   \n",
        "    \n",
        "    user_vert_att = Dense(1,trainable=trainable,use_bias=False,kernel_initializer=keras.initializers.Constant(value=np.zeros((400,1))))(user_subvert_rep)\n",
        "    user_vert_att = Reshape((len(subcategory_dict),))(user_vert_att) #(300,)\n",
        "    user_vert_att = Add()([user_vert_att,subvert_num_score]) #(300,)\n",
        "    \n",
        "    user_vert_att = RepeatVector(len(category_dict))(user_vert_att) #(18,300)\n",
        "    user_vert_att = Lambda(lambda x:x[0]-100*(1-x[1]))([user_vert_att,vert_subvert_mask_input]) #(18,300)\n",
        "    user_vert_att = Softmax()(user_vert_att)\n",
        "    \n",
        "    user_vert_rep = keras.layers.Dot(axes=[-1,-2])([user_vert_att,user_subvert_rep]) #(18,400)\n",
        "    user_vert_rep = CategoryEmbLayer(len(category_dict))(user_vert_rep) #(18,400)\n",
        "\n",
        "    user_global_att = Dense(1,trainable=trainable,use_bias=False,kernel_initializer=keras.initializers.Constant(value=np.zeros((400,1))))(user_vert_rep)\n",
        "    user_global_att = Reshape((len(category_dict),))(user_global_att) #(18,)\n",
        "\n",
        "    vert_num_emb = vert_num_embedding_layer(vert_num_input)\n",
        "    vert_num_score = vert_num_scorer(vert_num_emb)\n",
        "    vert_num_score = Reshape((len(category_dict),))(vert_num_score) #(18,1)   \n",
        "\n",
        "    user_global_att = Add()([user_global_att,vert_num_score]) #(18,)\n",
        "    user_global_att = Lambda(lambda x:x[0]-100*(1-x[1]))([user_global_att,clicked_vert_mask_input]) #(18,)\n",
        "    user_global_att = Softmax()(user_global_att)\n",
        "    \n",
        "        \n",
        "    user_global_rep = Dot(axes=[-1,-2])([user_global_att,user_vert_rep]) #(400,)\n",
        "    \n",
        "    return Model([clicked_title_input,clicked_vert_input,clicked_vert_mask_input,clicked_subvert_input,clicked_subvert_mask_input,vert_subvert_mask_input,vert_num_input,subvert_num_input],\n",
        "                 [user_subvert_rep,user_vert_rep,user_global_rep])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckar9PPwY0V6"
      },
      "source": [
        "def create_model(category_dict,subcategory_dict,title_word_embedding_matrix,entity_emb_matrix):\n",
        "    MAX_LENGTH = 35    \n",
        "    news_encoder = get_doc_encoder(title_word_embedding_matrix,entity_emb_matrix)\n",
        "\n",
        "    user_encoder = HirUserEncoder(category_dict,subcategory_dict)\n",
        "    \n",
        "    clicked_title_input = Input(shape=(50,35,), dtype='int32')\n",
        "    clicked_vert_input = Input(shape=(len(category_dict),50,), dtype='float32')\n",
        "    clicked_vert_mask_input = Input(shape=(len(category_dict),), dtype='float32')\n",
        "    clicked_subvert_input = Input(shape=(len(subcategory_dict),50,), dtype='float32')\n",
        "    clicked_subvert_mask_input = Input(shape=(len(subcategory_dict),), dtype='float32')\n",
        "    vert_subvert_mask_input = Input(shape=(len(category_dict),len(subcategory_dict)), dtype='float32')\n",
        "    \n",
        "    title_inputs = Input(shape=(1+npratio,35,),dtype='int32') \n",
        "    vert_inputs = Input(shape=(1+npratio,len(category_dict),),dtype='float32')  #(2,18)\n",
        "    subvert_inputs = Input(shape=(1+npratio,len(subcategory_dict),),dtype='float32')  #(2,18)\n",
        "    \n",
        "    vert_num_input = Input(shape=(len(category_dict),),dtype='int32')\n",
        "    subvert_num_input = Input(shape=(len(subcategory_dict),),dtype='int32')\n",
        "    \n",
        "    rw_vert_input = Input(shape=(1+npratio,),dtype='float32')\n",
        "    rw_subvert_input = Input(shape=(1+npratio,),dtype='float32')\n",
        "\n",
        "    clicked_title_vecs = TimeDistributed(news_encoder)(clicked_title_input)\n",
        "    news_vecs = TimeDistributed(news_encoder)(title_inputs)\n",
        "    \n",
        "    news_vecs = Dropout(0.25)(news_vecs)\n",
        "    clicked_title_vecs = Dropout(0.25)(clicked_title_vecs)\n",
        "\n",
        "    user_subvert_rep,user_vert_rep,user_global_rep = user_encoder([clicked_title_vecs,clicked_vert_input,clicked_vert_mask_input,clicked_subvert_input,clicked_subvert_mask_input,vert_subvert_mask_input,vert_num_input,subvert_num_input])\n",
        "    \n",
        "    \n",
        "    vs_user_vec = keras.layers.Dot(axes=(-1,-2))([vert_inputs,user_vert_rep]) #(batch_size,1+npratio,400)\n",
        "    svs_user_vec = keras.layers.Dot(axes=(-1,-2))([subvert_inputs,user_subvert_rep]) #(batch_size,1+npratio,400)\n",
        "\n",
        "\n",
        "    score1 = keras.layers.Dot(axes=-1)([news_vecs,user_global_rep])\n",
        "\n",
        "    vs_vecs = keras.layers.Concatenate(axis=-1)([news_vecs,vs_user_vec])\n",
        "    score2 = TimeDistributed(ConDot())(vs_vecs)\n",
        "    score2 = keras.layers.Reshape((1+npratio,))(score2)\n",
        "    \n",
        "    svs_vecs = keras.layers.Concatenate(axis=-1)([news_vecs,svs_user_vec])\n",
        "    score3 = TimeDistributed(ConDot())(svs_vecs)\n",
        "    score3 = keras.layers.Reshape((1+npratio,))(score3)\n",
        "    \n",
        "    \n",
        "    score2 = Multiply()([rw_vert_input,score2])\n",
        "    score3 = Multiply()([rw_subvert_input,score3])\n",
        "\n",
        "    rwer = Weighter()\n",
        "    scores = rwer([score1,score2,score3])\n",
        "    \n",
        "    \n",
        "    logits = keras.layers.Activation(keras.activations.softmax,name = 'recommend')(scores)     \n",
        "\n",
        "    model = Model([title_inputs,vert_inputs,subvert_inputs,\n",
        "                   clicked_title_input,clicked_vert_input,clicked_vert_mask_input,\n",
        "                   clicked_subvert_input,clicked_subvert_mask_input,\n",
        "                   vert_subvert_mask_input,vert_num_input,subvert_num_input,\n",
        "                  rw_vert_input,rw_subvert_input],logits) # max prob_click_positive\n",
        "    model.compile(loss=['categorical_crossentropy'],\n",
        "                  optimizer=Adam(lr=0.0001,amsgrad=True),\n",
        "                  metrics=['acc'])\n",
        "\n",
        "    \n",
        "    return model,news_encoder,user_encoder,rwer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSgzIkQKY4P7"
      },
      "source": [
        "### Pipelines (Main.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYZhobWygcPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd9b5dcc-2ac1-4eaf-f4d3-8b60e5889225"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPhRb5izY8hk"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras.backend as KTF\n",
        "# import tensorflow.compat.v1.keras.backend as KTF\n",
        "# config = tf.compat.v1.ConfigProto()\n",
        "# config = tf.ConfigProto()\n",
        "# config.gpu_options.allow_growth=True\n",
        "# session = tf.compat.v1.Session(config=config)\n",
        "# session = tf.Session(config=config)\n",
        " \n",
        "# KTF.set_session(session)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8r5BJ8Hc0J9"
      },
      "source": [
        "news,news_index,category_dict,subcategory_dict,word_dict = read_news(data_root_path,'docs.tsv')\n",
        "news_title,news_vert,news_subvert=get_doc_input(news,news_index,category_dict,subcategory_dict,word_dict)\n",
        "news_entity,news_entity_np,EntityId2Index = load_news_entity(news_index,KG_root_path)\n",
        "news_info = np.concatenate([news_title,news_entity_np],axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjfPW2cvc1j2"
      },
      "source": [
        "train_session = read_clickhistory(news_index,data_root_path,'train.tsv')\n",
        "train_user = parse_user(news_index,train_session)\n",
        "train_sess, train_user_id, train_label = get_train_input(news_index,train_session)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxsGN4HMc3l1"
      },
      "source": [
        "title_word_embedding_matrix, have_word = load_matrix(embedding_path,word_dict)\n",
        "entity_emb_matrix = load_entity_embedding(KG_root_path,EntityId2Index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4wufbcwc4hJ"
      },
      "source": [
        "index2nid = {}\n",
        "for nid, nix in news_index.items():\n",
        "    index2nid[nix] = nid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu3NIMw-c5Ut"
      },
      "source": [
        "vert_subvert_mask_table = np.zeros((1,len(category_dict),len(subcategory_dict)))\n",
        "for nid in range(1,len(news_vert)):\n",
        "    v = news_vert[nid]-1\n",
        "    sv = news_subvert[nid]-1\n",
        "    vert_subvert_mask_table[0,v,sv] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md6fhqIkc7Bl"
      },
      "source": [
        "from keras.utils import Sequence\n",
        "\n",
        "class get_hir_train_generator(Sequence):\n",
        "    def __init__(self,mask_prob,news_scoring,index2nid,news_vert, subvert,news_entity, news_entity_id, clicked_news,user_id, news_id, label, batch_size):\n",
        "        self.news_emb = news_scoring\n",
        "        self.vert = news_vert\n",
        "        self.subvert = subvert\n",
        "        self.entity = news_entity\n",
        "        self.entity_id = news_entity_id\n",
        "        self.index2nid = index2nid\n",
        "        \n",
        "        self.clicked_news = clicked_news\n",
        "\n",
        "        self.user_id = user_id\n",
        "        self.doc_id = news_id\n",
        "        self.label = label\n",
        "        \n",
        "        self.mask_prob = mask_prob\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.ImpNum = self.label.shape[0]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.ImpNum / float(self.batch_size)))\n",
        "    \n",
        "    def __get_news(self,docids):\n",
        "        news_emb = self.news_emb[docids]\n",
        "        vert = self.vert[docids]\n",
        "        subvert = self.subvert[docids]\n",
        "        entity = self.entity[docids]\n",
        "        return news_emb, vert, subvert, entity\n",
        "        \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx*self.batch_size\n",
        "        ed = (idx+1)*self.batch_size\n",
        "        if ed> self.ImpNum:\n",
        "            ed = self.ImpNum\n",
        "            \n",
        "        label = self.label[start:ed]\n",
        "\n",
        "        doc_ids = self.doc_id[start:ed]\n",
        "        title, vert, subvert, entity = self.__get_news(doc_ids)\n",
        "        \n",
        "        user_ids = self.user_id[start:ed]\n",
        "        clicked_ids = self.clicked_news[user_ids]\n",
        "        user_title, user_vert, user_subvert, user_entity = self.__get_news(clicked_ids)\n",
        "        \n",
        "        vert_subvert_mask_input = np.zeros((len(user_subvert),len(category_dict),len(subcategory_dict),))\n",
        "        for bid in range(len(user_subvert)):\n",
        "            for nid in range(len(user_subvert[bid])):\n",
        "                sv = user_subvert[bid][nid]\n",
        "                if sv ==0:\n",
        "                    continue\n",
        "                sv -= 1\n",
        "                vert_subvert_mask_input[bid,:,sv] = 1\n",
        "        vert_subvert_mask_input = vert_subvert_mask_input*vert_subvert_mask_table\n",
        "\n",
        "        \n",
        "        \n",
        "        user_vert = keras.utils.to_categorical(user_vert,len(category_dict)+1)\n",
        "        user_vert = user_vert.transpose((0,2,1))\n",
        "        user_vert = user_vert[:,1:,:]\n",
        "        user_vert_mask = user_vert.sum(axis=-1)\n",
        "        \n",
        "        vert = keras.utils.to_categorical(vert,len(category_dict)+1)\n",
        "        vert = vert[:,:,1:]\n",
        "        \n",
        "        user_subvert = keras.utils.to_categorical(user_subvert,len(subcategory_dict)+1)\n",
        "        user_subvert = user_subvert.transpose((0,2,1))\n",
        "        user_subvert = user_subvert[:,1:,:]\n",
        "        user_subvert_mask = user_subvert.sum(axis=-1)\n",
        "                \n",
        "        subvert = keras.utils.to_categorical(subvert,len(subcategory_dict)+1)\n",
        "        subvert = subvert[:,:,1:]\n",
        "    \n",
        "        user_vert_num = np.array(user_vert.sum(axis=-1),dtype='int32')\n",
        "        user_subvert_num = np.array(user_subvert.sum(axis=-1),dtype='int32')\n",
        "\n",
        "        user_subvert_mask = np.array(user_subvert_mask>0,dtype='float32')\n",
        "        user_vert_mask = np.array(user_vert_mask>0,dtype='float32')\n",
        "        vert_subvert_mask_input = np.array(vert_subvert_mask_input>0,dtype='float32')\n",
        "        \n",
        "        rw_vert = user_vert_num/(user_vert_num.sum(axis=-1).reshape((len(user_vert_num),1))+10**(-8)) #(bz,18)\n",
        "        rw_subvert = user_subvert_num/(user_subvert_num.sum(axis=-1).reshape((len(user_subvert_num),1))+10**(-8)) #(bz,300)\n",
        "        \n",
        "        \n",
        "        \n",
        "        rw_vert = rw_vert.reshape((rw_vert.shape[0],1,rw_vert.shape[1]))\n",
        "        rw_subvert = rw_subvert.reshape((rw_subvert.shape[0],1,rw_subvert.shape[1])) #(bz,1,18)\n",
        "        \n",
        "        rw_vert = (rw_vert*vert).sum(axis=-1)\n",
        "        rw_subvert = (rw_subvert*subvert).sum(axis=-1)\n",
        "        \n",
        "        train_mask = np.random.uniform(0,1,size=(ed-start,1)) > self.mask_prob\n",
        "        train_mask = np.array(train_mask,dtype='float32')\n",
        "        \n",
        "        rw_vert = rw_vert*train_mask\n",
        "        rw_subvert = rw_subvert*train_mask\n",
        "\n",
        "        return ([title,vert,subvert,user_title, user_vert,user_vert_mask,user_subvert,user_subvert_mask,vert_subvert_mask_input,user_vert_num,user_subvert_num,rw_vert,rw_subvert],[label])\n",
        "    \n",
        "    \n",
        "class get_hir_user_generator(Sequence):\n",
        "    def __init__(self,news_emb,news_vert,news_subvert,news_entity, clicked_news,batch_size):\n",
        "        self.news_emb = news_emb\n",
        "        self.vert = news_vert\n",
        "        self.subvert = news_subvert\n",
        "        self.entity = news_entity\n",
        "        \n",
        "        self.clicked_news = clicked_news\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.ImpNum = self.clicked_news.shape[0]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.ImpNum / float(self.batch_size)))\n",
        "    \n",
        "    \n",
        "    def __get_news(self,docids):\n",
        "        news_emb = self.news_emb[docids]\n",
        "        vert = self.vert[docids]\n",
        "        subvert = self.subvert[docids]\n",
        "        entity = self.entity[docids]\n",
        "        return news_emb, vert, subvert, entity\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        start = idx*self.batch_size\n",
        "        ed = (idx+1)*self.batch_size\n",
        "        if ed> self.ImpNum:\n",
        "            ed = self.ImpNum\n",
        "        \n",
        "        clicked_ids = self.clicked_news[start:ed]\n",
        "        user_title, user_vert, user_subvert, user_entity = self.__get_news(clicked_ids)\n",
        "        \n",
        "        vert_subvert_mask_input = np.zeros((len(user_subvert),len(category_dict),len(subcategory_dict),))\n",
        "        for bid in range(len(user_subvert)):\n",
        "            for nid in range(len(user_subvert[bid])):\n",
        "                sv = user_subvert[bid][nid]\n",
        "                if sv ==0:\n",
        "                    continue\n",
        "                sv -= 1\n",
        "                vert_subvert_mask_input[bid,:,sv] = 1\n",
        "        vert_subvert_mask_input = vert_subvert_mask_input*vert_subvert_mask_table\n",
        "\n",
        "        \n",
        "        \n",
        "        user_vert = keras.utils.to_categorical(user_vert,len(category_dict)+1)\n",
        "        user_vert = user_vert.transpose((0,2,1))\n",
        "        user_vert = user_vert[:,1:,:]\n",
        "        user_vert_mask = user_vert.sum(axis=-1)\n",
        "        \n",
        "        \n",
        "        user_subvert = keras.utils.to_categorical(user_subvert,len(subcategory_dict)+1)\n",
        "        user_subvert = user_subvert.transpose((0,2,1))\n",
        "        user_subvert = user_subvert[:,1:,:]\n",
        "        user_subvert_mask = user_subvert.sum(axis=-1)\n",
        "        \n",
        "        user_vert_num = np.array(user_vert.sum(axis=-1),dtype='int32')\n",
        "        user_subvert_num = np.array(user_subvert.sum(axis=-1),dtype='int32')\n",
        "        \n",
        "        user_subvert_mask = np.array(user_subvert_mask>0,dtype='float32')\n",
        "        user_vert_mask = np.array(user_vert_mask>0,dtype='float32')\n",
        "        vert_subvert_mask_input = np.array(vert_subvert_mask_input>0,dtype='float32')\n",
        "\n",
        "        return [user_title, user_vert,user_vert_mask,user_subvert,user_subvert_mask,vert_subvert_mask_input,user_vert_num,user_subvert_num]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model,news_encoder,user_encoder,rews = create_model(category_dict,subcategory_dict,title_word_embedding_matrix,entity_emb_matrix)"
      ],
      "metadata": {
        "id": "RJMR5PrIzimP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load checkpoint "
      ],
      "metadata": {
        "id": "y3vuhkWGzpCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpath = '12_13_hierec_v14_portion_30_60.hdf5'\n",
        "model.load_weights(checkpath)"
      ],
      "metadata": {
        "id": "4AkaK03-zi_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline (Continue)"
      ],
      "metadata": {
        "id": "HjtKktNvzylz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHTpgY7yc-qb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9e9703f-dde2-45c2-e1d2-e7bd77757b15"
      },
      "source": [
        "train_generator = get_hir_train_generator(0.9999,news_info,index2nid,news_vert,news_subvert,news_entity_np,news_entity,train_user['click'],train_user_id,train_sess,train_label,16)\n",
        "model.fit_generator(train_generator,epochs=1,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1/1\n",
            "94712/99215 [===========================>..] - ETA: 13:49 - loss: 1.2937 - acc: 0.4799"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save checkpoint "
      ],
      "metadata": {
        "id": "GhczrSvO1ISA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpath = '12_14_hierec_v14_portion_60_100.hdf5'\n",
        "model.save_weights(checkpath)"
      ],
      "metadata": {
        "id": "9VlmLw3U1Mnn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}