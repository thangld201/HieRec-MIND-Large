{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Tags",
    "colab": {
      "name": "Evaluation_HieRec_MIND_Large_20211.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "K4hUiZcdJF7L",
        "XXoX58I1ZWEn",
        "e6qfS4BsX-zI",
        "nK-8zRZ5kbGj",
        "oFb8Wo8PYKXw",
        "gdGkdPt9YVL_",
        "MI-FZ9OS2QmU"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv5BuZNfI-h-"
      },
      "source": [
        "!pip uninstall tensorflow==2.7.0 -y\n",
        "!pip install -U keras==2.2.4 tensorflow-gpu==1.14.0 h5py==2.10.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Raw Data"
      ],
      "metadata": {
        "id": "8HIHR4DICVoB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWfoqY4Ua-g3"
      },
      "source": [
        "# Large MIND \n",
        "!wget https://mind201910small.blob.core.windows.net/release/MINDlarge_train.zip -P data/train_set_large/\n",
        "!wget https://mind201910small.blob.core.windows.net/release/MINDlarge_dev.zip -P data/val_set_large/\n",
        "!wget https://mind201910small.blob.core.windows.net/release/MINDlarge_test.zip -P data/test_set_large/\n",
        "!unzip data/test_set_large/MINDlarge_test.zip -d data/test_set_large/\n",
        "!unzip data/train_set_large/MINDlarge_train.zip -d data/train_set_large/\n",
        "!unzip data/val_set_large/MINDlarge_dev.zip -d data/val_set_large/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeFXI6RCZuEa",
        "outputId": "0ce5b1c5-415a-41a6-bd11-847d62c5b65a"
      },
      "source": [
        "# GloVe\n",
        "!wget https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "!unzip /content/glove.840B.300d.zip -d /content/embedding/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-27 13:42:50--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
            "--2021-12-27 13:42:52--  http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768927 (2.0G) [application/zip]\n",
            "Saving to: ‘glove.840B.300d.zip’\n",
            "\n",
            "glove.840B.300d.zip 100%[===================>]   2.03G  5.04MB/s    in 6m 51s  \n",
            "\n",
            "2021-12-27 13:49:43 (5.05 MB/s) - ‘glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
            "\n",
            "Archive:  /content/glove.840B.300d.zip\n",
            "  inflating: /content/embedding/glove.840B.300d.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uejUZZ-JX4pW"
      },
      "source": [
        "### Hyperparams (hypers.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxPnjZhKKrha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb80b36e-b73b-481e-c1ee-cfd535e45177"
      },
      "source": [
        "# %tensorflow_version 1.x\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)\n",
        "print(tensorflow.test.gpu_device_name())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.14.0\n",
            "/device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAdisPGGX8wo"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "npratio = 4\n",
        "MAX_SENTENCE = 30\n",
        "MAX_ALL = 50\n",
        "MAX_SENT_LENGTH=30\n",
        "MAX_SENTS=50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4hUiZcdJF7L"
      },
      "source": [
        "### Additional functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAIKQmJxJNpn"
      },
      "source": [
        "def get_raw_entity_list_dict():\n",
        "    raw_entity_list_dict = []\n",
        "    set_news = set()\n",
        "\n",
        "    with open('/content/data/train_set_large/news.tsv') as f:\n",
        "            a_lines = f.readlines()\n",
        "    with open('/content/data/val_set_large/news.tsv') as f:\n",
        "            a_lines += f.readlines()\n",
        "    with open('/content/data/test_set_large/news.tsv') as f:\n",
        "            a_lines += f.readlines()\n",
        "\n",
        "    for l in a_lines:\n",
        "        l = l.strip().split('\\t')\n",
        "        if l[0] not in set_news:\n",
        "            raw_entity_list_dict.append({\"doc_id\":l[0],\"entities\":json.loads(l[-2])})\n",
        "            set_news.add(l[0])\n",
        "    return raw_entity_list_dict\n",
        "\n",
        "\n",
        "def mini_get_raw_entity_list_dict():\n",
        "    raw_entity_list_dict = []\n",
        "    set_news = set()\n",
        "\n",
        "    with open('/content/data/train_set/news.tsv') as f:\n",
        "            a_lines = f.readlines()\n",
        "    with open('/content/data/val_set/news.tsv') as f:\n",
        "            a_lines += f.readlines()\n",
        "\n",
        "    for l in a_lines:\n",
        "        l = l.strip().split('\\t')\n",
        "        if l[0] not in set_news:\n",
        "            raw_entity_list_dict.append({\"doc_id\":l[0],\"entities\":json.loads(l[-2])})\n",
        "            set_news.add(l[0])\n",
        "    return raw_entity_list_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwqSzvrmPK00"
      },
      "source": [
        "def get_raw_pretrained_entity_embedding():\n",
        "    import numpy as np\n",
        "    raw_entity_embedding = {}\n",
        "    set_entity = set()\n",
        "\n",
        "    with open('/content/data/train_set_large/entity_embedding.vec','r') as f:\n",
        "        tmp_a = f.readlines()\n",
        "\n",
        "    with open('/content/data/val_set_large/entity_embedding.vec','r') as f:\n",
        "        tmp_a += f.readlines()\n",
        "\n",
        "    with open('/content/data/test_set_large/entity_embedding.vec','r') as f:\n",
        "        tmp_a += f.readlines()\n",
        "\n",
        "    for l in tmp_a:\n",
        "        l = l.strip().split('\\t')\n",
        "        if l[0] not in set_entity:\n",
        "            set_entity.add(l[0])\n",
        "            raw_entity_embedding[l[0]] = np.array(l[1:]).astype(np.float)\n",
        "\n",
        "    return raw_entity_embedding\n",
        "\n",
        "\n",
        "def mini_get_raw_pretrained_entity_embedding():\n",
        "    import numpy as np\n",
        "    raw_entity_embedding = {}\n",
        "    set_entity = set()\n",
        "\n",
        "    with open('data/train_set/entity_embedding.vec','r') as f:\n",
        "        tmp_a = f.readlines()\n",
        "\n",
        "    with open('/content/data/val_set/entity_embedding.vec','r') as f:\n",
        "        tmp_a += f.readlines()\n",
        "        \n",
        "    for l in tmp_a:\n",
        "        l = l.strip().split('\\t')\n",
        "        if l[0] not in set_entity:\n",
        "            set_entity.add(l[0])\n",
        "            raw_entity_embedding[l[0]] = np.array(l[1:]).astype(np.float)\n",
        "\n",
        "    return raw_entity_embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXoX58I1ZWEn"
      },
      "source": [
        "### Data preparation (ProcessRawData.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MIND Large\n",
        "raw_data_path = './data'\n",
        "output_data_path = './input_data'\n",
        "\n",
        "if os.path.exists(output_data_path) == False:\n",
        "  os.mkdir(output_data_path)\n",
        "\n",
        "\n",
        "with open(os.path.join(raw_data_path,'train_set_large','news.tsv')) as f:\n",
        "    news1 = f.readlines()\n",
        "with open(os.path.join(raw_data_path,'val_set_large','news.tsv')) as f:\n",
        "    news2 = f.readlines()\n",
        "with open(os.path.join(raw_data_path,'test_set_large','news.tsv')) as f:\n",
        "    news3 = f.readlines()\n",
        "\n",
        "news = []\n",
        "news_dict = {}\n",
        "for l in news1 + news2 + news3:\n",
        "    nid = l.strip('\\n').split('\\t')[0]\n",
        "    if not nid in news_dict:\n",
        "        news_dict[nid] = 1\n",
        "        news.append(l)\n",
        "\n",
        "with open(os.path.join(output_data_path,'docs.tsv'),'w') as f:\n",
        "    for i in range(len(news)):\n",
        "        f.write(news[i])\n",
        "\n",
        "with open(os.path.join(raw_data_path,'test_set_large','behaviors.tsv')) as f:\n",
        "    behaviors3 = f.readlines()\n",
        "\n",
        "test_behaviors = []\n",
        "\n",
        "\n",
        "# Get test data\n",
        "# Split into portions\n",
        "test_behaviors = behaviors3\n",
        "\n",
        "start_portion = int(len(test_behaviors)*0)\n",
        "end_portion = int(len(test_behaviors)*0.25)\n",
        "test_behaviors = test_behaviors[start_portion:end_portion]\n",
        "\n",
        "print(f\"Testing start from portion {start_portion} to {end_portion}\")\n",
        "\n",
        "with open(os.path.join(output_data_path,'test.tsv'),'w') as f:\n",
        "    for i in range(len(test_behaviors)):\n",
        "        f.write(test_behaviors[i])"
      ],
      "metadata": {
        "id": "PTvdFB0ksuot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08ea36fc-7f13-4ea5-e61b-cf8cd907f9df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing start from portion 0 to 592681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6qfS4BsX-zI"
      },
      "source": [
        "### Preprocessing (preprocessing.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzRMYXxlZ16S"
      },
      "source": [
        "data_root_path = '/content/input_data/'\n",
        "embedding_path = '/content/embedding/'\n",
        "KG_root_path = '/content/HieRec_KGData'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BHIuYIsYDH-"
      },
      "source": [
        "from datetime import datetime\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "import json\n",
        "\n",
        "def trans2tsp(timestr):\n",
        "    return int(time.mktime(datetime.strptime(timestr, '%m/%d/%Y %I:%M:%S %p').timetuple()))\n",
        "\n",
        "def newsample(nnn,ratio):\n",
        "    if ratio >len(nnn):\n",
        "        return random.sample(nnn*(ratio//len(nnn)+1),ratio)\n",
        "    else:\n",
        "        return random.sample(nnn,ratio)\n",
        "\n",
        "def shuffle(pn,labeler,pos):\n",
        "    index=np.arange(pn.shape[0])\n",
        "    pn=pn[index]\n",
        "    labeler=labeler[index]\n",
        "    pos=pos[index]\n",
        "    \n",
        "    for i in range(pn.shape[0]):\n",
        "        index=np.arange(npratio+1)\n",
        "        pn[i,:]=pn[i,index]\n",
        "        labeler[i,:]=labeler[i,index]\n",
        "    return pn,labeler,pos\n",
        "\n",
        "def read_news(path,filenames):\n",
        "    news={}\n",
        "    category=[]\n",
        "    subcategory=[]\n",
        "    news_index={}\n",
        "    index=1\n",
        "    word_dict={}\n",
        "    word_index=1\n",
        "    with open(os.path.join(path,filenames)) as f:\n",
        "        lines=f.readlines()\n",
        "    for line in lines:\n",
        "        splited = line.strip('\\n').split('\\t')\n",
        "        doc_id,vert,subvert,title= splited[0:4]\n",
        "        news_index[doc_id]=index\n",
        "        index+=1\n",
        "        category.append(vert)\n",
        "        subcategory.append(subvert)\n",
        "        title = title.lower()\n",
        "        title=word_tokenize(title)\n",
        "        news[doc_id]=[vert,subvert,title]\n",
        "        for word in title:\n",
        "            word = word.lower()\n",
        "            if not(word in word_dict):\n",
        "                word_dict[word]=word_index\n",
        "                word_index+=1\n",
        "    category=list(set(category))\n",
        "    subcategory=list(set(subcategory))\n",
        "    category_dict={}\n",
        "    index=1\n",
        "    for c in category:\n",
        "        category_dict[c]=index\n",
        "        index+=1\n",
        "    subcategory_dict={}\n",
        "    index=1\n",
        "    for c in subcategory:\n",
        "        subcategory_dict[c]=index\n",
        "        index+=1\n",
        "    return news,news_index,category_dict,subcategory_dict,word_dict\n",
        "\n",
        "def get_doc_input(news,news_index,category,subcategory,word_dict):\n",
        "    news_num=len(news)+1\n",
        "    news_title=np.zeros((news_num,MAX_SENTENCE),dtype='int32')\n",
        "    news_vert=np.zeros((news_num,),dtype='int32')\n",
        "    news_subvert=np.zeros((news_num,),dtype='int32')\n",
        "    for key in news:    \n",
        "        vert,subvert,title=news[key]\n",
        "        doc_index=news_index[key]\n",
        "        news_vert[doc_index]=category[vert]\n",
        "        news_subvert[doc_index]=subcategory[subvert]\n",
        "        for word_id in range(min(MAX_SENTENCE,len(title))):\n",
        "            news_title[doc_index,word_id]=word_dict[title[word_id].lower()]\n",
        "    return news_title,news_vert,news_subvert\n",
        "\n",
        "def load_matrix(embedding_path,word_dict):\n",
        "    embedding_matrix = np.zeros((len(word_dict)+1,300))\n",
        "    have_word=[]\n",
        "    with open(os.path.join(embedding_path,'glove.840B.300d.txt'),'rb') as f:\n",
        "        while True:\n",
        "            l=f.readline()\n",
        "            if len(l)==0:\n",
        "                break\n",
        "            l=l.split()\n",
        "            word = l[0].decode()\n",
        "            if word in word_dict:\n",
        "                index = word_dict[word]\n",
        "                tp = [float(x) for x in l[1:]]\n",
        "                embedding_matrix[index]=np.array(tp)\n",
        "                have_word.append(word)\n",
        "    return embedding_matrix,have_word\n",
        "\n",
        "def read_clickhistory(news_index,data_root_path,filename):\n",
        "    \n",
        "    lines = []\n",
        "    userids = []\n",
        "    with open(os.path.join(data_root_path,filename)) as f:\n",
        "        lines = f.readlines()\n",
        "        \n",
        "    sessions = []\n",
        "    for i in range(len(lines)):\n",
        "        _,uid,eventime, click, imps = lines[i].strip().split('\\t')\n",
        "        if click == '':\n",
        "            clikcs = []\n",
        "        else:\n",
        "            clikcs = click.split()\n",
        "        true_click = []\n",
        "        for click in clikcs:\n",
        "            if not click in news_index:\n",
        "                continue\n",
        "            true_click.append(click)\n",
        "        pos = []\n",
        "        neg = []\n",
        "        for imp in imps.split():\n",
        "            docid, label = imp.split('-')\n",
        "            if label == '1':\n",
        "                pos.append(docid)\n",
        "            else:\n",
        "                neg.append(docid)\n",
        "        sessions.append([true_click,pos,neg])\n",
        "    return sessions\n",
        "\n",
        "def parse_user(news_index,session):\n",
        "    user_num = len(session)\n",
        "    user={'click': np.zeros((user_num,MAX_ALL),dtype='int32'),}\n",
        "    for user_id in range(len(session)):\n",
        "        tclick = []\n",
        "        click, pos, neg =session[user_id]\n",
        "        for i in range(len(click)):\n",
        "            tclick.append(news_index[click[i]])\n",
        "        click = tclick\n",
        "\n",
        "        if len(click) >MAX_ALL:\n",
        "            click = click[-MAX_ALL:]\n",
        "        else:\n",
        "            click=[0]*(MAX_ALL-len(click)) + click\n",
        "            \n",
        "        user['click'][user_id] = np.array(click)\n",
        "    return user\n",
        "\n",
        "def get_train_input(news_index,session):\n",
        "    sess_pos = []\n",
        "    sess_neg = []\n",
        "    user_id = []\n",
        "    for sess_id in range(len(session)):\n",
        "        sess = session[sess_id]\n",
        "        _, poss, negs=sess\n",
        "        for i in range(len(poss)):\n",
        "            pos = poss[i]\n",
        "            neg=newsample(negs,npratio)\n",
        "            sess_pos.append(pos)\n",
        "            sess_neg.append(neg)\n",
        "            user_id.append(sess_id)\n",
        "    sess_all = np.zeros((len(sess_pos),1+npratio),dtype='int32')\n",
        "    label = np.zeros((len(sess_pos),1+npratio))\n",
        "    for sess_id in range(sess_all.shape[0]):\n",
        "        pos = sess_pos[sess_id]\n",
        "        negs = sess_neg[sess_id]\n",
        "        sess_all[sess_id,0] = news_index[pos]\n",
        "        index = 1\n",
        "        for neg in negs:\n",
        "            sess_all[sess_id,index] = news_index[neg]\n",
        "            index+=1\n",
        "        label[sess_id,0]=1\n",
        "    user_id = np.array(user_id, dtype='int32')\n",
        "    \n",
        "    return sess_all, user_id, label\n",
        "\n",
        "def get_test_input(news_index,session):\n",
        "    \n",
        "    Impressions = []\n",
        "    userid = []\n",
        "    for sess_id in range(len(session)):\n",
        "        _, poss, negs = session[sess_id]\n",
        "        imp = {'labels':[],\n",
        "                'docs':[]}\n",
        "        userid.append(sess_id)\n",
        "        for i in range(len(poss)):\n",
        "            docid = news_index[poss[i]]\n",
        "            imp['docs'].append(docid)\n",
        "            imp['labels'].append(1)\n",
        "        for i in range(len(negs)):\n",
        "            docid = news_index[negs[i]]\n",
        "            imp['docs'].append(docid)\n",
        "            imp['labels'].append(0)\n",
        "        Impressions.append(imp)\n",
        "        \n",
        "    userid = np.array(userid,dtype='int32')\n",
        "    \n",
        "    return Impressions, userid,\n",
        "\n",
        "def load_news_entity(news_index,KG_root_path):\n",
        "    # CHANGE FROM HERE\n",
        "    # with open(os.path.join(KG_root_path,'Release_Small_title.tsv')) as f:\n",
        "        # lines = f.readlines()\n",
        "    \n",
        "    # lines = mini_get_raw_entity_list_dict()\n",
        "    # 12_12 switch to MIND Large\n",
        "    lines = get_raw_entity_list_dict()\n",
        "    \n",
        "    EntityId2Index = {}\n",
        "    ctt = 1\n",
        "    \n",
        "    news_entity = {}\n",
        "    g = []\n",
        "    for i in range(len(lines)):\n",
        "        d = lines[i]\n",
        "        # d = json.loads(lines[i].strip('\\n'))\n",
        "    # CHANGE ENDS HERE\n",
        "        docid = d['doc_id']\n",
        "        if not docid in news_index:\n",
        "            continue\n",
        "        news_entity[docid] = []\n",
        "        entities = d['entities']\n",
        "        for j in range(len(entities)):\n",
        "            e = entities[j]['Label']\n",
        "            eid = entities[j]['WikidataId']\n",
        "            if not eid in EntityId2Index:\n",
        "                EntityId2Index[eid] = ctt\n",
        "                ctt += 1    \n",
        "            news_entity[docid].append([e,eid,EntityId2Index[eid]])\n",
        "    \n",
        "    meta_news_entity = {}\n",
        "    news_entity2 = {}\n",
        "    \n",
        "    \n",
        "    news_entity_id = {}\n",
        "    for nid in news_entity:\n",
        "        news_entity_id[nid] = []\n",
        "        for e in news_entity[nid]:\n",
        "            news_entity_id[nid].append(e[-2])\n",
        "        news_entity_id[nid] = set(news_entity_id[nid])\n",
        "        \n",
        "    \n",
        "    for docid in news_entity:\n",
        "        meta_news_entity[docid] = news_entity[docid]\n",
        "        news_entity2[docid] = []\n",
        "        for v in news_entity[docid]:\n",
        "            news_entity2[docid].append(v[-1])\n",
        "        news_entity2[docid] = list(set(news_entity2[docid]))[:5]\n",
        "        news_entity2[docid] = news_entity2[docid] + [0]*(5-len(news_entity2[docid]))\n",
        "        news_entity2[docid] = np.array(news_entity2[docid])\n",
        "    \n",
        "    news_entity_np = np.zeros((len(news_entity2)+1,5),dtype='int32')\n",
        "    for nid in news_index:\n",
        "        nix = news_index[nid]\n",
        "        news_entity_np[nix] = news_entity2[nid]\n",
        "        \n",
        "    return news_entity_id,news_entity_np,EntityId2Index\n",
        "\n",
        "def load_entity_embedding(KG_root_path,EntityId2Index):\n",
        "    entity_emb = np.zeros((len(EntityId2Index)+1,100))\n",
        "    # CHANGE STARTS HERE\n",
        "\n",
        "    # import pickle\n",
        "    # with open(os.path.join(KG_root_path,'title_entity_emb.pkl'),'rb') as f:\n",
        "        # title_entity_emb = pickle.load(f)\n",
        "    # title_entity_emb = mini_get_raw_pretrained_entity_embedding()\n",
        "    # 12_12 switch to MIND Large\n",
        "    title_entity_emb = get_raw_pretrained_entity_embedding()\n",
        "    \n",
        "    for eid in EntityId2Index:\n",
        "        if eid in title_entity_emb:\n",
        "            eix = EntityId2Index[eid]\n",
        "            entity_emb[eix] = title_entity_emb[eid]\n",
        "    # CHANGE ENDS HERE\n",
        "    return entity_emb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MIND Large Behaviors Input Function"
      ],
      "metadata": {
        "id": "nK-8zRZ5kbGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add functions for MIND Large Test Behaviors\n",
        "def read_clickhistory_MIND_large(news_index,data_root_path,filename):\n",
        "    \n",
        "    lines = []\n",
        "    userids = []\n",
        "    with open(os.path.join(data_root_path,filename)) as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    sessions = []\n",
        "    for i in range(len(lines)):\n",
        "        imp_id,uid,eventime, click, imps = lines[i].strip().split('\\t')\n",
        "        if click == '':\n",
        "            clikcs = []\n",
        "        else:\n",
        "            clikcs = click.split()\n",
        "        true_click = []\n",
        "        for click in clikcs:\n",
        "            if not click in news_index:\n",
        "                continue\n",
        "            true_click.append(click)\n",
        "        pos = []\n",
        "        neg = []\n",
        "        # Test data has no label -> default with 0\n",
        "        for imp in imps.split():\n",
        "            docid = imp\n",
        "            neg.append(docid)\n",
        "        # first field is imp_id, 4 fields in total\n",
        "        sessions.append([imp_id,true_click,pos,neg])\n",
        "    return sessions\n",
        "\n",
        "def parse_user_MIND_large(news_index,session):\n",
        "    user_num = len(session)\n",
        "    user={'click': np.zeros((user_num,MAX_ALL),dtype='int32'),\n",
        "          'history': np.ones((user_num,), dtype='int32'),\n",
        "          'imp_id': ['id']*user_num}\n",
        "    for user_id in range(len(session)):\n",
        "        tclick = []\n",
        "        # All inputs are in neg\n",
        "        imp_id,click, pos, neg =session[user_id]\n",
        "        for i in range(len(click)):\n",
        "            tclick.append(news_index[click[i]])\n",
        "        click = tclick\n",
        "\n",
        "        if len(click) >MAX_ALL:\n",
        "            click = click[-MAX_ALL:]\n",
        "        else:\n",
        "            click=[0]*(MAX_ALL-len(click)) + click\n",
        "        \n",
        "        # Add imp_id and history (False -> No history), later we use this for popularity ranking\n",
        "        user['click'][user_id] = np.array(click)\n",
        "        user['imp_id'][user_id] = imp_id\n",
        "        if len(click)==0:\n",
        "          user['history'][user_id]=0\n",
        "        \n",
        "    return user\n",
        "\n",
        "def get_test_input_MIND_large(news_index,session):\n",
        "    \n",
        "    Impressions = []\n",
        "    userid = []\n",
        "    for sess_id in range(len(session)):\n",
        "        imp_id, click_hist, poss, negs = session[sess_id]\n",
        "        # only docs and imp_id make sense\n",
        "        imp = {'labels':[],\n",
        "                'docs':[],\n",
        "               'real_doc_ids':[],\n",
        "               'have_hist':True,\n",
        "               'imp_id':imp_id}\n",
        "        userid.append(sess_id)\n",
        "        for i in range(len(negs)):\n",
        "            docid = news_index[negs[i]]\n",
        "            imp['docs'].append(docid)\n",
        "            # 12_15 added\n",
        "            imp['real_doc_ids'].append(negs[i])\n",
        "            imp['labels'].append(0)\n",
        "            if len(click_hist)==0 or click_hist is None:\n",
        "                imp['have_hist']=False\n",
        "        Impressions.append(imp)\n",
        "        \n",
        "    userid = np.array(userid,dtype='int32')\n",
        "    \n",
        "    return Impressions, userid,"
      ],
      "metadata": {
        "id": "AkSkCt6OcxVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # MIND Small testing\n",
        "# def read_clickhistory_v2(news_index,data_root_path,filename):\n",
        "    \n",
        "#     lines = []\n",
        "#     userids = []\n",
        "#     with open(os.path.join(data_root_path,filename)) as f:\n",
        "#         lines = f.readlines()\n",
        "        \n",
        "#     sessions = []\n",
        "#     for i in range(len(lines)):\n",
        "#         imp_id,uid,eventime, click, imps = lines[i].strip().split('\\t')\n",
        "#         if click == '':\n",
        "#             clikcs = []\n",
        "#         else:\n",
        "#             clikcs = click.split()\n",
        "#         true_click = []\n",
        "#         for click in clikcs:\n",
        "#             if not click in news_index:\n",
        "#                 continue\n",
        "#             true_click.append(click)\n",
        "#         pos = []\n",
        "#         neg = []\n",
        "#         for imp in imps.split():\n",
        "#             docid, label = imp.split('-')\n",
        "#             neg.append(docid)\n",
        "#         sessions.append([imp_id,true_click,pos,neg])\n",
        "#     return sessions\n",
        "\n",
        "# def parse_user_v2(news_index,session):\n",
        "#     user_num = len(session)\n",
        "#     user={'click': np.zeros((user_num,MAX_ALL),dtype='int32'),\n",
        "#           'history': np.ones((user_num,),dtype='int32'),\n",
        "#           'imp_id': ['id']*user_num}\n",
        "#     for user_id in range(len(session)):\n",
        "#         tclick = []\n",
        "#         imp_id, click, pos, neg =session[user_id]\n",
        "#         for i in range(len(click)):\n",
        "#             tclick.append(news_index[click[i]])\n",
        "#         click = tclick\n",
        "\n",
        "#         if len(click) >MAX_ALL:\n",
        "#             click = click[-MAX_ALL:]\n",
        "#         else:\n",
        "#             click=[0]*(MAX_ALL-len(click)) + click\n",
        "            \n",
        "#         user['click'][user_id] = np.array(click)\n",
        "#         user['imp_id'][user_id] = imp_id\n",
        "#         if len(click)==0:\n",
        "#           user['history'][user_id] = 0\n",
        "#     return user\n",
        "\n",
        "# def get_test_input_v2(news_index,session):\n",
        "    \n",
        "#     Impressions = []\n",
        "#     userid = []\n",
        "#     for sess_id in range(len(session)):\n",
        "#         imp_id, click_hist, poss, negs = session[sess_id]\n",
        "#         imp = {'labels':[],\n",
        "#                 'docs':[],\n",
        "#                'real_doc_ids':[],\n",
        "#                'have_hist': True,\n",
        "#                'imp_id':imp_id}\n",
        "#         userid.append(sess_id)\n",
        "#         for i in range(len(negs)):\n",
        "#             docid = news_index[negs[i]]\n",
        "#             imp['docs'].append(docid)\n",
        "#             imp['real_doc_ids'].append(negs[i])\n",
        "#             imp['labels'].append(0)\n",
        "#             if len(click_hist)==0 or click_hist is None:\n",
        "#                 imp['have_hist']=False\n",
        "\n",
        "#         Impressions.append(imp)\n",
        "        \n",
        "#     userid = np.array(userid,dtype='int32')\n",
        "    \n",
        "#     return Impressions, userid,"
      ],
      "metadata": {
        "id": "cJJ7f_NupFLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFb8Wo8PYKXw"
      },
      "source": [
        "### Utils (utils.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eg4VupZYLj-"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "def dcg_score(y_true, y_score, k=10):\n",
        "    order = np.argsort(y_score)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "    gains = 2 ** y_true - 1\n",
        "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
        "    return np.sum(gains / discounts)\n",
        "\n",
        "\n",
        "def ndcg_score(y_true, y_score, k=10):\n",
        "    best = dcg_score(y_true, y_true, k)\n",
        "    actual = dcg_score(y_true, y_score, k)\n",
        "    return actual / best\n",
        "\n",
        "\n",
        "def mrr_score(y_true, y_score):\n",
        "    order = np.argsort(y_score)[::-1]\n",
        "    y_true = np.take(y_true, order)\n",
        "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
        "    return np.sum(rr_score) / np.sum(y_true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdGkdPt9YVL_"
      },
      "source": [
        "### Models (models.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvwBdVLHYZrn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d441cc47-2872-4244-f3d0-7d977f602e59"
      },
      "source": [
        "import numpy\n",
        "import keras\n",
        "from keras.utils.np_utils import *\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "from keras.layers import Embedding, concatenate\n",
        "from keras.layers import Dense, Input, Flatten, average,Lambda\n",
        "\n",
        "from keras.layers import *\n",
        "from keras.models import Model, load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer, InputSpec\n",
        "from keras import initializers #keras2\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from keras.optimizers import *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VfkPfQvYhD4"
      },
      "source": [
        "class Attention(Layer):\n",
        " \n",
        "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
        "        self.nb_head = nb_head\n",
        "        self.size_per_head = size_per_head\n",
        "        self.output_dim = nb_head*size_per_head\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        " \n",
        "    def build(self, input_shape):\n",
        "        self.WQ = self.add_weight(name='WQ',\n",
        "                                  shape=(input_shape[0][-1], self.output_dim),\n",
        "                                  initializer='glorot_uniform',\n",
        "                                  trainable=True)\n",
        "        self.WK = self.add_weight(name='WK',\n",
        "                                  shape=(input_shape[1][-1], self.output_dim),\n",
        "                                  initializer='glorot_uniform',\n",
        "                                  trainable=True)\n",
        "        self.WV = self.add_weight(name='WV',\n",
        "                                  shape=(input_shape[2][-1], self.output_dim),\n",
        "                                  initializer='glorot_uniform',\n",
        "                                  trainable=True)\n",
        "        super(Attention, self).build(input_shape)\n",
        " \n",
        "    def Mask(self, inputs, seq_len, mode='mul'):\n",
        "        if seq_len == None:\n",
        "            return inputs\n",
        "        else:\n",
        "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
        "            mask = 1 - K.cumsum(mask, 1)\n",
        "            for _ in range(len(inputs.shape)-2):\n",
        "                mask = K.expand_dims(mask, 2)\n",
        "            if mode == 'mul':\n",
        "                return inputs * mask\n",
        "            if mode == 'add':\n",
        "                return inputs - (1 - mask) * 1e12\n",
        " \n",
        "    def call(self, x):\n",
        "        if len(x) == 3:\n",
        "            Q_seq,K_seq,V_seq = x\n",
        "            Q_len,V_len = None,None\n",
        "        elif len(x) == 5:\n",
        "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
        "\n",
        "        Q_seq = K.dot(Q_seq, self.WQ)\n",
        "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
        "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
        "        K_seq = K.dot(K_seq, self.WK)\n",
        "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
        "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
        "        V_seq = K.dot(V_seq, self.WV)\n",
        "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
        "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
        "\n",
        "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
        "        A = K.permute_dimensions(A, (0,3,2,1))\n",
        "        A = self.Mask(A, V_len, 'add')\n",
        "        A = K.permute_dimensions(A, (0,3,2,1))\n",
        "        A = K.softmax(A)\n",
        "\n",
        "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n",
        "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n",
        "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n",
        "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
        "        return O_seq\n",
        " \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0][0], input_shape[0][1], self.output_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuAD6sonYlqj"
      },
      "source": [
        "def AttentivePooling(dim1,dim2):\n",
        "    vecs_input = Input(shape=(dim1,dim2),dtype='float32')\n",
        "    user_vecs =Dropout(0.2)(vecs_input)\n",
        "    user_att = Dense(200,activation='tanh')(user_vecs)\n",
        "    user_att = keras.layers.Flatten()(Dense(1)(user_att))\n",
        "    user_att = Activation('softmax')(user_att)\n",
        "    user_vec = keras.layers.Dot((1,1))([user_vecs,user_att])\n",
        "    model = Model(vecs_input,user_vec)\n",
        "    return model\n",
        "\n",
        "def ConDot():\n",
        "    vec_input = keras.layers.Input(shape=(400*2,))\n",
        "    vec1 = keras.layers.Lambda(lambda x:x[:,:400])(vec_input)\n",
        "    vec2 = keras.layers.Lambda(lambda x:x[:,400:])(vec_input)\n",
        "    score = keras.layers.Dot(axes=-1)([vec1,vec2])\n",
        "    return Model(vec_input,score)\n",
        "\n",
        "def get_doc_encoder(title_word_embedding_matrix,entity_emb_matrix):\n",
        "\n",
        "    news_input = Input(shape=(35,),dtype='int32')\n",
        "    \n",
        "    \n",
        "    sentence_input = keras.layers.Lambda(lambda x:x[:,:30])(news_input)\n",
        "    title_word_embedding_layer = Embedding(title_word_embedding_matrix.shape[0], 300, weights=[title_word_embedding_matrix],trainable=True)\n",
        "    word_vecs = title_word_embedding_layer(sentence_input)\n",
        "    droped_vecs = Dropout(0.2)(word_vecs)\n",
        "    word_rep = Attention(20,20)([droped_vecs]*3)\n",
        "    droped_rep = Dropout(0.2)(word_rep)\n",
        "    title_vec = AttentivePooling(30,400)(droped_rep)\n",
        "    \n",
        "    entity_input = keras.layers.Lambda(lambda x:x[:,30:])(news_input)\n",
        "    entity_embedding_layer = Embedding(entity_emb_matrix.shape[0], 100, weights=[entity_emb_matrix],trainable=True)\n",
        "    entity_vecs = entity_embedding_layer(entity_input)\n",
        "    droped_vecs = Dropout(0.2)(entity_vecs)\n",
        "    entity_rep = Attention(5,20)([droped_vecs]*3)\n",
        "    droped_rep = Dropout(0.2)(entity_rep)\n",
        "    entity_vec = AttentivePooling(5,100)(droped_rep)\n",
        "    \n",
        "    vec = keras.layers.Concatenate(axis=-1)([title_vec,entity_vec])\n",
        "    vec = keras.layers.Dense(400)(vec)\n",
        "    \n",
        "    \n",
        "    sentEncodert = Model(news_input, vec)\n",
        "    return sentEncodert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2zqDMPhYsQa"
      },
      "source": [
        "class CategoryEmbLayer(Layer):\n",
        "    \n",
        "    def __init__(self,n, **kwargs):\n",
        "        super(CategoryEmbLayer, self).__init__(**kwargs)\n",
        "        self.n = n\n",
        "        \n",
        "     \n",
        "    def build(self, input_shape):\n",
        "        trainable = True\n",
        "        if self.n>1:\n",
        "            self.W = self.add_weight(name='W',\n",
        "                                  shape=(self.n,400),\n",
        "                                  initializer=keras.initializers.Constant(value=np.zeros((self.n,400))),\n",
        "                                  trainable=trainable)\n",
        "        else:\n",
        "            self.W = self.add_weight(name='W',\n",
        "                                  shape=(400,),\n",
        "                                  initializer=keras.initializers.Constant(value=np.zeros((400,))),\n",
        "                                  trainable=trainable)\n",
        "            \n",
        "    def call(self,x):\n",
        "        return x+self.W\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "\n",
        "class Weighter(Layer):\n",
        "     \n",
        "    def __init__(self, **kwargs):\n",
        "        super(Weighter, self).__init__(**kwargs)\n",
        "        \n",
        "\n",
        "     \n",
        "    def build(self, input_shape):\n",
        "        trainable = False\n",
        "        self.w1 = self.add_weight(name='w1',\n",
        "                                  shape=(1,),\n",
        "                                  initializer=keras.initializers.Constant(value=0.15/0.15),\n",
        "                                  trainable=trainable)\n",
        "\n",
        "        self.w2 = self.add_weight(name='w2',\n",
        "                                  shape=(1,),\n",
        "                                  initializer=keras.initializers.Constant(value=0.15/0.15),\n",
        "                                  trainable=trainable)\n",
        "        \n",
        "        self.w3 = self.add_weight(name='w3',\n",
        "                                  shape=(1,),\n",
        "                                  initializer=keras.initializers.Constant(value=0.7/0.15),\n",
        "                                  trainable=trainable)\n",
        "\n",
        "        \n",
        "        super(Weighter, self).build(input_shape)\n",
        "        \n",
        "        \n",
        "    def call(self,x):\n",
        "\n",
        "        return self.w1*x[0]+self.w2*x[1]+self.w3*x[2]\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "\n",
        "        return input_shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StRHfPfBYyUl"
      },
      "source": [
        "def HirUserEncoder(category_dict,subcategory_dict):\n",
        "    \n",
        "    AttTrainable = True\n",
        "    \n",
        "    clicked_title_input = Input(shape=(50,400,), dtype='float32')\n",
        "    \n",
        "    clicked_vert_input = Input(shape=(len(category_dict),50,), dtype='float32')\n",
        "    clicked_vert_mask_input = Input(shape=(len(category_dict),), dtype='float32')\n",
        "    \n",
        "    clicked_subvert_input = Input(shape=(len(subcategory_dict),50,), dtype='float32')\n",
        "    clicked_subvert_mask_input = Input(shape=(len(subcategory_dict),), dtype='float32')\n",
        "    \n",
        "    vert_subvert_mask_input = Input(shape=(len(category_dict),len(subcategory_dict)),dtype='float32')\n",
        "\n",
        "    vert_num_input = Input(shape=(len(category_dict),),dtype='int32')\n",
        "    subvert_num_input = Input(shape=(len(subcategory_dict),),dtype='int32')\n",
        "\n",
        "    subvert_num_embedding_layer = Embedding(51, 128,trainable=True)\n",
        "    subvert_num_scorer = Dense(1)\n",
        "\n",
        "\n",
        "    vert_num_embedding_layer = subvert_num_embedding_layer #Embedding(51, 128,trainable=True)\n",
        "    vert_num_scorer = subvert_num_scorer\n",
        "\n",
        "    title_vecs = clicked_title_input\n",
        "    \n",
        "    trainable = True\n",
        "    \n",
        "    user_subvert_att = Dense(1,trainable=trainable,use_bias=False,kernel_initializer=keras.initializers.Constant(value=np.zeros((400,1))),)(title_vecs)\n",
        "\n",
        "    user_subvert_att = keras.layers.Reshape((50,))(user_subvert_att)\n",
        "    user_subvert_att = keras.layers.RepeatVector(len(subcategory_dict))(user_subvert_att)\n",
        "    user_subvert_att = keras.layers.Lambda(lambda x:x[0]-100*(1-x[1]))([user_subvert_att,clicked_subvert_input])    \n",
        "    user_subvert_att = keras.layers.Activation('softmax')(user_subvert_att) #(300,50)\n",
        "\n",
        "    user_subvert_att = keras.layers.Lambda(lambda x:x[0]*x[1])([user_subvert_att,clicked_subvert_input]) #(300,400)\n",
        "    user_subvert_rep = keras.layers.Dot(axes=[-1,-2])([user_subvert_att,title_vecs]) #（300,400)\n",
        "    user_subvert_rep = CategoryEmbLayer(len(subcategory_dict))(user_subvert_rep)  #（300,400) \n",
        "    \n",
        "    \n",
        "    subvert_num_emb = subvert_num_embedding_layer(subvert_num_input)\n",
        "    subvert_num_score = subvert_num_scorer(subvert_num_emb)\n",
        "    subvert_num_score = Reshape((len(subcategory_dict),))(subvert_num_score) #(300,)   \n",
        "    \n",
        "    user_vert_att = Dense(1,trainable=trainable,use_bias=False,kernel_initializer=keras.initializers.Constant(value=np.zeros((400,1))))(user_subvert_rep)\n",
        "    user_vert_att = Reshape((len(subcategory_dict),))(user_vert_att) #(300,)\n",
        "    user_vert_att = Add()([user_vert_att,subvert_num_score]) #(300,)\n",
        "    \n",
        "    user_vert_att = RepeatVector(len(category_dict))(user_vert_att) #(18,300)\n",
        "    user_vert_att = Lambda(lambda x:x[0]-100*(1-x[1]))([user_vert_att,vert_subvert_mask_input]) #(18,300)\n",
        "    user_vert_att = Softmax()(user_vert_att)\n",
        "    \n",
        "    user_vert_rep = keras.layers.Dot(axes=[-1,-2])([user_vert_att,user_subvert_rep]) #(18,400)\n",
        "    user_vert_rep = CategoryEmbLayer(len(category_dict))(user_vert_rep) #(18,400)\n",
        "\n",
        "    user_global_att = Dense(1,trainable=trainable,use_bias=False,kernel_initializer=keras.initializers.Constant(value=np.zeros((400,1))))(user_vert_rep)\n",
        "    user_global_att = Reshape((len(category_dict),))(user_global_att) #(18,)\n",
        "\n",
        "    vert_num_emb = vert_num_embedding_layer(vert_num_input)\n",
        "    vert_num_score = vert_num_scorer(vert_num_emb)\n",
        "    vert_num_score = Reshape((len(category_dict),))(vert_num_score) #(18,1)   \n",
        "\n",
        "    user_global_att = Add()([user_global_att,vert_num_score]) #(18,)\n",
        "    user_global_att = Lambda(lambda x:x[0]-100*(1-x[1]))([user_global_att,clicked_vert_mask_input]) #(18,)\n",
        "    user_global_att = Softmax()(user_global_att)\n",
        "    \n",
        "        \n",
        "    user_global_rep = Dot(axes=[-1,-2])([user_global_att,user_vert_rep]) #(400,)\n",
        "    \n",
        "    return Model([clicked_title_input,clicked_vert_input,clicked_vert_mask_input,clicked_subvert_input,clicked_subvert_mask_input,vert_subvert_mask_input,vert_num_input,subvert_num_input],\n",
        "                 [user_subvert_rep,user_vert_rep,user_global_rep])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckar9PPwY0V6"
      },
      "source": [
        "def create_model(category_dict,subcategory_dict,title_word_embedding_matrix,entity_emb_matrix):\n",
        "    MAX_LENGTH = 35    \n",
        "    news_encoder = get_doc_encoder(title_word_embedding_matrix,entity_emb_matrix)\n",
        "\n",
        "    user_encoder = HirUserEncoder(category_dict,subcategory_dict)\n",
        "    \n",
        "    clicked_title_input = Input(shape=(50,35,), dtype='int32')\n",
        "    clicked_vert_input = Input(shape=(len(category_dict),50,), dtype='float32')\n",
        "    clicked_vert_mask_input = Input(shape=(len(category_dict),), dtype='float32')\n",
        "    clicked_subvert_input = Input(shape=(len(subcategory_dict),50,), dtype='float32')\n",
        "    clicked_subvert_mask_input = Input(shape=(len(subcategory_dict),), dtype='float32')\n",
        "    vert_subvert_mask_input = Input(shape=(len(category_dict),len(subcategory_dict)), dtype='float32')\n",
        "    \n",
        "    title_inputs = Input(shape=(1+npratio,35,),dtype='int32') \n",
        "    vert_inputs = Input(shape=(1+npratio,len(category_dict),),dtype='float32')  #(2,18)\n",
        "    subvert_inputs = Input(shape=(1+npratio,len(subcategory_dict),),dtype='float32')  #(2,18)\n",
        "    \n",
        "    vert_num_input = Input(shape=(len(category_dict),),dtype='int32')\n",
        "    subvert_num_input = Input(shape=(len(subcategory_dict),),dtype='int32')\n",
        "    \n",
        "    rw_vert_input = Input(shape=(1+npratio,),dtype='float32')\n",
        "    rw_subvert_input = Input(shape=(1+npratio,),dtype='float32')\n",
        "\n",
        "    clicked_title_vecs = TimeDistributed(news_encoder)(clicked_title_input)\n",
        "    news_vecs = TimeDistributed(news_encoder)(title_inputs)\n",
        "    \n",
        "    news_vecs = Dropout(0.25)(news_vecs)\n",
        "    clicked_title_vecs = Dropout(0.25)(clicked_title_vecs)\n",
        "\n",
        "    user_subvert_rep,user_vert_rep,user_global_rep = user_encoder([clicked_title_vecs,clicked_vert_input,clicked_vert_mask_input,clicked_subvert_input,clicked_subvert_mask_input,vert_subvert_mask_input,vert_num_input,subvert_num_input])\n",
        "    \n",
        "    \n",
        "    vs_user_vec = keras.layers.Dot(axes=(-1,-2))([vert_inputs,user_vert_rep]) #(batch_size,1+npratio,400)\n",
        "    svs_user_vec = keras.layers.Dot(axes=(-1,-2))([subvert_inputs,user_subvert_rep]) #(batch_size,1+npratio,400)\n",
        "\n",
        "\n",
        "    score1 = keras.layers.Dot(axes=-1)([news_vecs,user_global_rep])\n",
        "\n",
        "    vs_vecs = keras.layers.Concatenate(axis=-1)([news_vecs,vs_user_vec])\n",
        "    score2 = TimeDistributed(ConDot())(vs_vecs)\n",
        "    score2 = keras.layers.Reshape((1+npratio,))(score2)\n",
        "    \n",
        "    svs_vecs = keras.layers.Concatenate(axis=-1)([news_vecs,svs_user_vec])\n",
        "    score3 = TimeDistributed(ConDot())(svs_vecs)\n",
        "    score3 = keras.layers.Reshape((1+npratio,))(score3)\n",
        "    \n",
        "    \n",
        "    score2 = Multiply()([rw_vert_input,score2])\n",
        "    score3 = Multiply()([rw_subvert_input,score3])\n",
        "\n",
        "    rwer = Weighter()\n",
        "    scores = rwer([score1,score2,score3])\n",
        "    \n",
        "    \n",
        "    logits = keras.layers.Activation(keras.activations.softmax,name = 'recommend')(scores)     \n",
        "\n",
        "    model = Model([title_inputs,vert_inputs,subvert_inputs,\n",
        "                   clicked_title_input,clicked_vert_input,clicked_vert_mask_input,\n",
        "                   clicked_subvert_input,clicked_subvert_mask_input,\n",
        "                   vert_subvert_mask_input,vert_num_input,subvert_num_input,\n",
        "                  rw_vert_input,rw_subvert_input],logits) # max prob_click_positive\n",
        "    model.compile(loss=['categorical_crossentropy'],\n",
        "                  optimizer=Adam(lr=0.0001,amsgrad=True),\n",
        "                  metrics=['acc'])\n",
        "\n",
        "    \n",
        "    return model,news_encoder,user_encoder,rwer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Pipelines"
      ],
      "metadata": {
        "id": "MI-FZ9OS2QmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "wmsqn1MO2anX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea760348-ed51-4eb2-c3a1-fc86be9c8afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras.backend as KTF"
      ],
      "metadata": {
        "id": "VKsPQXBp2cky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news,news_index,category_dict,subcategory_dict,word_dict = read_news(data_root_path,'docs.tsv')\n",
        "news_title,news_vert,news_subvert=get_doc_input(news,news_index,category_dict,subcategory_dict,word_dict)\n",
        "news_entity,news_entity_np,EntityId2Index = load_news_entity(news_index,KG_root_path)\n",
        "news_info = np.concatenate([news_title,news_entity_np],axis=-1)"
      ],
      "metadata": {
        "id": "YH1TxsHE2kCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpgzmQHS2qpl"
      },
      "source": [
        "title_word_embedding_matrix, have_word = load_matrix(embedding_path,word_dict)\n",
        "entity_emb_matrix = load_entity_embedding(KG_root_path,EntityId2Index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4loFCWKs2qpl"
      },
      "source": [
        "index2nid = {}\n",
        "for nid, nix in news_index.items():\n",
        "    index2nid[nix] = nid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfTw7CKE2qpm"
      },
      "source": [
        "vert_subvert_mask_table = np.zeros((1,len(category_dict),len(subcategory_dict)))\n",
        "for nid in range(1,len(news_vert)):\n",
        "    v = news_vert[nid]-1\n",
        "    sv = news_subvert[nid]-1\n",
        "    vert_subvert_mask_table[0,v,sv] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_session =read_clickhistory_MIND_large(news_index,data_root_path,'test.tsv')\n",
        "test_user = parse_user_MIND_large(news_index,test_session)\n",
        "test_impressions, test_userids = get_test_input_MIND_large(news_index,test_session)"
      ],
      "metadata": {
        "id": "sNUBeTE12trR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilities"
      ],
      "metadata": {
        "id": "JJkYmx8-6tm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilities for ranking\n",
        "import numpy as np\n",
        "\n",
        "def get_sort_order(input_score):\n",
        "    real_order = [0]*len(input_score)\n",
        "    tmp_order = np.argsort(input_score)[::-1]\n",
        "    for i in range(len(input_score)):\n",
        "      real_order[tmp_order[i]] = i+1\n",
        "    return real_order\n",
        "\n",
        "\n",
        "def get_default_ranking(input_score):\n",
        "    return [a+1 for a in range(len(input_score))]"
      ],
      "metadata": {
        "id": "QqtN7Xdt6vmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm3-khpi2w9O"
      },
      "source": [
        "from keras.utils import Sequence\n",
        "\n",
        "class get_hir_train_generator(Sequence):\n",
        "    def __init__(self,mask_prob,news_scoring,index2nid,news_vert, subvert,news_entity, news_entity_id, clicked_news,user_id, news_id, label, batch_size):\n",
        "        self.news_emb = news_scoring\n",
        "        self.vert = news_vert\n",
        "        self.subvert = subvert\n",
        "        self.entity = news_entity\n",
        "        self.entity_id = news_entity_id\n",
        "        self.index2nid = index2nid\n",
        "        \n",
        "        self.clicked_news = clicked_news\n",
        "\n",
        "        self.user_id = user_id\n",
        "        self.doc_id = news_id\n",
        "        self.label = label\n",
        "        \n",
        "        self.mask_prob = mask_prob\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.ImpNum = self.label.shape[0]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.ImpNum / float(self.batch_size)))\n",
        "    \n",
        "    def __get_news(self,docids):\n",
        "        news_emb = self.news_emb[docids]\n",
        "        vert = self.vert[docids]\n",
        "        subvert = self.subvert[docids]\n",
        "        entity = self.entity[docids]\n",
        "        return news_emb, vert, subvert, entity\n",
        "        \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx*self.batch_size\n",
        "        ed = (idx+1)*self.batch_size\n",
        "        if ed> self.ImpNum:\n",
        "            ed = self.ImpNum\n",
        "            \n",
        "        label = self.label[start:ed]\n",
        "\n",
        "        doc_ids = self.doc_id[start:ed]\n",
        "        title, vert, subvert, entity = self.__get_news(doc_ids)\n",
        "        \n",
        "        user_ids = self.user_id[start:ed]\n",
        "        clicked_ids = self.clicked_news[user_ids]\n",
        "        user_title, user_vert, user_subvert, user_entity = self.__get_news(clicked_ids)\n",
        "        \n",
        "        vert_subvert_mask_input = np.zeros((len(user_subvert),len(category_dict),len(subcategory_dict),))\n",
        "        for bid in range(len(user_subvert)):\n",
        "            for nid in range(len(user_subvert[bid])):\n",
        "                sv = user_subvert[bid][nid]\n",
        "                if sv ==0:\n",
        "                    continue\n",
        "                sv -= 1\n",
        "                vert_subvert_mask_input[bid,:,sv] = 1\n",
        "        vert_subvert_mask_input = vert_subvert_mask_input*vert_subvert_mask_table\n",
        "\n",
        "        \n",
        "        \n",
        "        user_vert = keras.utils.to_categorical(user_vert,len(category_dict)+1)\n",
        "        user_vert = user_vert.transpose((0,2,1))\n",
        "        user_vert = user_vert[:,1:,:]\n",
        "        user_vert_mask = user_vert.sum(axis=-1)\n",
        "        \n",
        "        vert = keras.utils.to_categorical(vert,len(category_dict)+1)\n",
        "        vert = vert[:,:,1:]\n",
        "        \n",
        "        user_subvert = keras.utils.to_categorical(user_subvert,len(subcategory_dict)+1)\n",
        "        user_subvert = user_subvert.transpose((0,2,1))\n",
        "        user_subvert = user_subvert[:,1:,:]\n",
        "        user_subvert_mask = user_subvert.sum(axis=-1)\n",
        "                \n",
        "        subvert = keras.utils.to_categorical(subvert,len(subcategory_dict)+1)\n",
        "        subvert = subvert[:,:,1:]\n",
        "    \n",
        "        user_vert_num = np.array(user_vert.sum(axis=-1),dtype='int32')\n",
        "        user_subvert_num = np.array(user_subvert.sum(axis=-1),dtype='int32')\n",
        "\n",
        "        user_subvert_mask = np.array(user_subvert_mask>0,dtype='float32')\n",
        "        user_vert_mask = np.array(user_vert_mask>0,dtype='float32')\n",
        "        vert_subvert_mask_input = np.array(vert_subvert_mask_input>0,dtype='float32')\n",
        "        \n",
        "        rw_vert = user_vert_num/(user_vert_num.sum(axis=-1).reshape((len(user_vert_num),1))+10**(-8)) #(bz,18)\n",
        "        rw_subvert = user_subvert_num/(user_subvert_num.sum(axis=-1).reshape((len(user_subvert_num),1))+10**(-8)) #(bz,300)\n",
        "        \n",
        "        \n",
        "        \n",
        "        rw_vert = rw_vert.reshape((rw_vert.shape[0],1,rw_vert.shape[1]))\n",
        "        rw_subvert = rw_subvert.reshape((rw_subvert.shape[0],1,rw_subvert.shape[1])) #(bz,1,18)\n",
        "        \n",
        "        rw_vert = (rw_vert*vert).sum(axis=-1)\n",
        "        rw_subvert = (rw_subvert*subvert).sum(axis=-1)\n",
        "        \n",
        "        train_mask = np.random.uniform(0,1,size=(ed-start,1)) > self.mask_prob\n",
        "        train_mask = np.array(train_mask,dtype='float32')\n",
        "        \n",
        "        rw_vert = rw_vert*train_mask\n",
        "        rw_subvert = rw_subvert*train_mask\n",
        "\n",
        "\n",
        "\n",
        "        return ([title,vert,subvert,user_title, user_vert,user_vert_mask,user_subvert,user_subvert_mask,vert_subvert_mask_input,user_vert_num,user_subvert_num,rw_vert,rw_subvert],[label])\n",
        "    \n",
        "    \n",
        "class get_hir_user_generator(Sequence):\n",
        "    def __init__(self,news_emb,news_vert,news_subvert,news_entity, clicked_news,batch_size):\n",
        "        self.news_emb = news_emb\n",
        "        self.vert = news_vert\n",
        "        self.subvert = news_subvert\n",
        "        self.entity = news_entity\n",
        "        \n",
        "        self.clicked_news = clicked_news\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.ImpNum = self.clicked_news.shape[0]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.ImpNum / float(self.batch_size)))\n",
        "    \n",
        "    \n",
        "    def __get_news(self,docids):\n",
        "        news_emb = self.news_emb[docids]\n",
        "        vert = self.vert[docids]\n",
        "        subvert = self.subvert[docids]\n",
        "        entity = self.entity[docids]\n",
        "        return news_emb, vert, subvert, entity\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        start = idx*self.batch_size\n",
        "        ed = (idx+1)*self.batch_size\n",
        "        if ed> self.ImpNum:\n",
        "            ed = self.ImpNum\n",
        "        \n",
        "        clicked_ids = self.clicked_news[start:ed]\n",
        "        user_title, user_vert, user_subvert, user_entity = self.__get_news(clicked_ids)\n",
        "        \n",
        "        vert_subvert_mask_input = np.zeros((len(user_subvert),len(category_dict),len(subcategory_dict),))\n",
        "        for bid in range(len(user_subvert)):\n",
        "            for nid in range(len(user_subvert[bid])):\n",
        "                sv = user_subvert[bid][nid]\n",
        "                if sv ==0:\n",
        "                    continue\n",
        "                sv -= 1\n",
        "                vert_subvert_mask_input[bid,:,sv] = 1\n",
        "        vert_subvert_mask_input = vert_subvert_mask_input*vert_subvert_mask_table\n",
        "\n",
        "        \n",
        "        \n",
        "        user_vert = keras.utils.to_categorical(user_vert,len(category_dict)+1)\n",
        "        user_vert = user_vert.transpose((0,2,1))\n",
        "        user_vert = user_vert[:,1:,:]\n",
        "        user_vert_mask = user_vert.sum(axis=-1)\n",
        "        \n",
        "        \n",
        "        user_subvert = keras.utils.to_categorical(user_subvert,len(subcategory_dict)+1)\n",
        "        user_subvert = user_subvert.transpose((0,2,1))\n",
        "        user_subvert = user_subvert[:,1:,:]\n",
        "        user_subvert_mask = user_subvert.sum(axis=-1)\n",
        "        \n",
        "        user_vert_num = np.array(user_vert.sum(axis=-1),dtype='int32')\n",
        "        user_subvert_num = np.array(user_subvert.sum(axis=-1),dtype='int32')\n",
        "        \n",
        "        user_subvert_mask = np.array(user_subvert_mask>0,dtype='float32')\n",
        "        user_vert_mask = np.array(user_vert_mask>0,dtype='float32')\n",
        "        vert_subvert_mask_input = np.array(vert_subvert_mask_input>0,dtype='float32')\n",
        "\n",
        "        return [user_title, user_vert,user_vert_mask,user_subvert,user_subvert_mask,vert_subvert_mask_input,user_vert_num,user_subvert_num]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline (Continue)"
      ],
      "metadata": {
        "id": "P1XFPSwJ2w9N"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtqGSzII2w9P"
      },
      "source": [
        "def evaluate_combine2(test_impressions,users,user_subvert_rep,user_vert_rep,user_global_rep,w1,w2,w3,ranking_data):\n",
        "    for i in range(len(test_impressions)):\n",
        "        imp_id = test_impressions[i]['imp_id']\n",
        "        real_doc_ids = test_impressions[i]['real_doc_ids']\n",
        "        have_hist = test_impressions[i]['have_hist']\n",
        "        labels = test_impressions[i]['labels']\n",
        "        nids = test_impressions[i]['docs']\n",
        "\n",
        "        verts = news_vert[nids]\n",
        "        verts = verts-1\n",
        "        subverts = news_subvert[nids]\n",
        "        subverts = subverts-1\n",
        "\n",
        "        user_gv = user_global_rep[i]\n",
        "        user_vv = user_vert_rep[i]\n",
        "        user_svv = user_subvert_rep[i]\n",
        "\n",
        "        click = users[i]\n",
        "        \n",
        "        nv = news_scoring[nids]\n",
        "        score1 = np.dot(nv,user_gv)\n",
        "        user_vv = user_vv[verts]\n",
        "        score2 = (nv*user_vv).sum(axis=-1)\n",
        "        \n",
        "        mask2 = []\n",
        "        for v in verts:\n",
        "            t = news_vert[click]==(v+1)\n",
        "            mask2.append(t.sum())\n",
        "        mask2 = np.array(mask2)\n",
        "        mask2 = mask2/((click>0).sum()+10**(-6))\n",
        "        \n",
        "        \n",
        "        user_svv = user_svv[subverts]\n",
        "        score3 = (nv*user_svv).sum(axis=-1)\n",
        "\n",
        "        mask3 = []\n",
        "        for svi in range(len(subverts)):\n",
        "            sv = subverts[svi]\n",
        "            t = (news_subvert[click]==(sv+1))\n",
        "            mask3.append(t.sum())\n",
        "        mask3 = np.array(mask3)\n",
        "        mask3 = mask3/((click>0).sum()+10**(-6))\n",
        "        \n",
        "        score1 = np.array(score1)\n",
        "        score2 = np.array(score2)\n",
        "        score3 = np.array(score3)\n",
        "\n",
        "        score = score1*w1+mask2*score2*w2+mask3*score3*w3\n",
        "        \n",
        "        if have_hist:\n",
        "            ranked_score = get_sort_order(score)\n",
        "        else:\n",
        "            ranked_score = get_default_ranking(score)\n",
        "        \n",
        "        # if not have_hist:\n",
        "        #     print(\"=\"*10)\n",
        "        #     print(imp_id)\n",
        "        #     print(real_doc_ids)\n",
        "        #     print(score)\n",
        "        #     print(ranked_score)\n",
        "        #     print(have_hist)\n",
        "\n",
        "        ranking_data.append({'imp_id':imp_id,\n",
        "                             'ranked_score':list(ranked_score),\n",
        "                             'real_doc_ids':list(real_doc_ids),\n",
        "                             'float_score':list(score),\n",
        "                             'have_hist':have_hist})\n",
        "\n",
        "    return ranking_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Init Model and Load Checkpoint"
      ],
      "metadata": {
        "id": "yL-1HEz224Sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model,news_encoder,user_encoder,rews = create_model(category_dict,subcategory_dict,title_word_embedding_matrix,entity_emb_matrix)"
      ],
      "metadata": {
        "id": "hOFu0IHH2w9Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f537e60-bee8-4aaf-8a10-6a4f16a35b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpath = '/content/drive/MyDrive/12_26_HieRec_Weights/12_26_hierec_v14_portion_175_200_1e6.hdf5'\n",
        "model.load_weights(checkpath)"
      ],
      "metadata": {
        "id": "wn1bdGrt24Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_scoring = news_encoder.predict(news_info,verbose=True)"
      ],
      "metadata": {
        "id": "taWElE_761UK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbbfbcbc-aa00-4c6f-dceb-507bd79cb77a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130380/130380 [==============================] - 18s 142us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Begin Evaluation"
      ],
      "metadata": {
        "id": "R-GrQJvCN3xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "t0 = time.time()"
      ],
      "metadata": {
        "id": "Ko9fi4xJHjuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranking_data = []\n",
        "for i in range(int(np.ceil(len(test_user['click'])/100))):\n",
        "    start = i*100\n",
        "    ed = (i+1)*100\n",
        "    ed = min(ed,len(test_user['click']))\n",
        "    test_user_generator = get_hir_user_generator(news_scoring,news_vert,news_subvert,news_entity_np,test_user['click'][start:ed],32)\n",
        "    user_subvert_rep,user_vert_rep,user_global_rep = user_encoder.predict_generator(test_user_generator,verbose=False)\n",
        "    ranking_data = evaluate_combine2(test_impressions[start:ed],test_user['click'][start:ed],user_subvert_rep,user_vert_rep,user_global_rep,0.15,0.15,0.7,ranking_data=ranking_data)"
      ],
      "metadata": {
        "id": "FnyjIlLy69AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Testing on 25% of data took {0:.2f} minutes\".format((time.time()-t0)/60))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_LZIpsOHngJ",
        "outputId": "767d9687-d120-416f-886a-898a9188c2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing on 25% of data took 30.85 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ranking_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xUs17jVK14D",
        "outputId": "679bba2b-00a4-44f2-c50b-bea7f5dad588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'imp_id': '1', 'ranked_score': [4, 13, 15, 11, 10, 7, 16, 8, 2, 1, 12, 5, 3, 6, 14, 9], 'real_doc_ids': ['N101071', 'N15647', 'N83400', 'N124838', 'N57092', 'N64623', 'N62785', 'N112133', 'N98744', 'N55764', 'N16531', 'N54103', 'N128905', 'N2296', 'N45689', 'N87027'], 'float_score': [0.11867101202268489, -0.1312348924894668, -0.27543838456913866, 0.030484646908702513, 0.03884781495537609, 0.0790591619915266, -0.286534006363054, 0.069807264154889, 0.30891214789008414, 0.821328967492867, 0.018681509751921115, 0.11509675902669966, 0.2724157039769365, 0.09129255045142404, -0.1314573734998703, 0.06870223752475761], 'have_hist': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12_14 added\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "class NpEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        if isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        return super(NpEncoder, self).default(obj)\n",
        "\n",
        "def output_json_ranking_file(ranking_data, output_json_name='12_14_submissions.json'):\n",
        "    with open(output_json_name, \"w\",encoding='utf8') as output_file:\n",
        "            json.dump(ranking_data,output_file,ensure_ascii=False,cls=NpEncoder)\n",
        "\n",
        "\n",
        "def get_data_from_path(path):\n",
        "    f = open(path,'r')\n",
        "    data_1 = json.load(f)\n",
        "    f.close()\n",
        "    return data_1"
      ],
      "metadata": {
        "id": "ftZaFherBoSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save ranking_data\n",
        "# output_json_ranking_file(ranking_data=ranking_data,output_json_name='default_ranking_data_portion_0_25.json')\n",
        "# output_json_ranking_file(ranking_data=ranking_data,output_json_name='default_ranking_data_portion_25_50.json')\n",
        "# output_json_ranking_file(ranking_data=ranking_data,output_json_name='default_ranking_data_portion_50_75.json')\n",
        "# output_json_ranking_file(ranking_data=ranking_data,output_json_name='default_ranking_data_portion_75_100.json')"
      ],
      "metadata": {
        "id": "SrCtMXCSGSO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ranking_data))\n",
        "# compare, ranking_data must have len equal\n",
        "print(end_portion-start_portion)\n",
        "print(f\"Testing of Impressions ID from {start_portion} to {end_portion}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e19oZq4tDefD",
        "outputId": "a3f74c10-4a58-4fc7-f989-f8c21ba89010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "592681\n",
            "592681\n",
            "Testing of Impressions ID from 0 to 592681\n"
          ]
        }
      ]
    }
  ]
}